name: Comprehensive Testing Automation (London TDD + 100% Coverage)

on:
  push:
    branches: [ main, develop, feature/*, hotfix/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 2 * * *' # Daily at 2 AM UTC for comprehensive regression testing
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
        - quick
        - full
        - regression
        - performance
      coverage_threshold:
        description: 'Coverage threshold (0-100)'
        required: false
        default: '100'
        type: string
      enable_mutation_testing:
        description: 'Enable mutation testing'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '20'
  CACHE_VERSION: 'v5'
  COVERAGE_THRESHOLD: ${{ github.event.inputs.coverage_threshold || '100' }}
  TEST_LEVEL: ${{ github.event.inputs.test_level || 'full' }}
  MUTATION_TESTING: ${{ github.event.inputs.enable_mutation_testing || 'false' }}
  
  # London TDD Configuration
  TDD_MODE: 'london'
  TDD_STRICT: true
  FAIL_FAST: true
  
  # Performance optimization
  CI: true
  NODE_ENV: test
  NODE_OPTIONS: '--max-old-space-size=8192'
  JEST_WORKERS: '75%'

jobs:
  # ===== TEST PLANNING & VALIDATION =====
  test-planning:
    name: Test Planning & Validation
    runs-on: ubuntu-latest
    timeout-minutes: 3
    outputs:
      test-matrix: ${{ steps.test-matrix.outputs.matrix }}
      coverage-threshold: ${{ steps.thresholds.outputs.coverage }}
      should-run-mutation: ${{ steps.validation.outputs.mutation }}
      test-categories: ${{ steps.categories.outputs.list }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2 # For change detection

      - name: Analyze code changes for intelligent test selection
        id: changes
        run: |
          echo "Analyzing code changes for intelligent test selection..."
          
          # Get changed files
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            changed_files=$(git diff --name-only HEAD^ HEAD)
          else
            changed_files=$(git diff --name-only HEAD~1 HEAD)
          fi
          
          echo "Changed files:"
          echo "$changed_files"
          
          # Categorize changes
          backend_changed=false
          frontend_changed=false
          config_changed=false
          docs_changed=false
          
          echo "$changed_files" | while read -r file; do
            case "$file" in
              backend/*) backend_changed=true ;;
              frontend/*) frontend_changed=true ;;
              *.yml|*.yaml|*.json|docker-compose*|Dockerfile*) config_changed=true ;;
              *.md|docs/*) docs_changed=true ;;
            esac
          done
          
          echo "backend-changed=$backend_changed" >> $GITHUB_OUTPUT
          echo "frontend-changed=$frontend_changed" >> $GITHUB_OUTPUT
          echo "config-changed=$config_changed" >> $GITHUB_OUTPUT
          echo "docs-changed=$docs_changed" >> $GITHUB_OUTPUT

      - name: Generate intelligent test matrix
        id: test-matrix
        run: |
          echo "Generating intelligent test matrix based on changes and test level..."
          
          matrix='{
            "include": [
              {
                "test-type": "unit",
                "timeout": 10,
                "parallel": true,
                "coverage-required": true,
                "os": "ubuntu-latest"
              },
              {
                "test-type": "integration", 
                "timeout": 15,
                "parallel": true,
                "coverage-required": true,
                "os": "ubuntu-latest"
              },
              {
                "test-type": "e2e",
                "timeout": 20,
                "parallel": false,
                "coverage-required": false,
                "os": "ubuntu-latest"
              }
            ]
          }'
          
          # Add performance tests for main branch or explicit request
          if [[ "${{ github.ref }}" == "refs/heads/main" || "${{ env.TEST_LEVEL }}" == "performance" || "${{ env.TEST_LEVEL }}" == "full" ]]; then
            matrix=$(echo "$matrix" | jq '.include += [{
              "test-type": "performance",
              "timeout": 25,
              "parallel": false,
              "coverage-required": false,
              "os": "ubuntu-latest"
            }]')
          fi
          
          # Add security tests for production branches
          if [[ "${{ github.ref }}" == "refs/heads/main" || "${{ env.TEST_LEVEL }}" == "full" ]]; then
            matrix=$(echo "$matrix" | jq '.include += [{
              "test-type": "security",
              "timeout": 15,
              "parallel": true,
              "coverage-required": false,
              "os": "ubuntu-latest"
            }]')
          fi
          
          echo "matrix=$matrix" >> $GITHUB_OUTPUT
          echo "Test matrix generated successfully"

      - name: Set coverage thresholds dynamically
        id: thresholds
        run: |
          case "${{ env.TEST_LEVEL }}" in
            "quick")
              coverage_threshold=85
              ;;
            "full"|"regression")
              coverage_threshold=100
              ;;
            "performance")
              coverage_threshold=90
              ;;
            *)
              coverage_threshold=${{ env.COVERAGE_THRESHOLD }}
              ;;
          esac
          
          echo "coverage=$coverage_threshold" >> $GITHUB_OUTPUT
          echo "Coverage threshold set to: $coverage_threshold%"

      - name: Validate test configuration
        id: validation
        run: |
          echo "Validating test configuration..."
          
          # Validate coverage threshold
          if [[ ${{ steps.thresholds.outputs.coverage }} -lt 50 || ${{ steps.thresholds.outputs.coverage }} -gt 100 ]]; then
            echo "❌ Invalid coverage threshold: ${{ steps.thresholds.outputs.coverage }}%"
            exit 1
          fi
          
          # Determine if mutation testing should run
          mutation_enabled=false
          if [[ "${{ env.MUTATION_TESTING }}" == "true" && ("${{ github.ref }}" == "refs/heads/main" || "${{ env.TEST_LEVEL }}" == "full") ]]; then
            mutation_enabled=true
          fi
          
          echo "mutation=$mutation_enabled" >> $GITHUB_OUTPUT
          echo "✅ Test configuration validated successfully"

      - name: Define test categories
        id: categories
        run: |
          categories='[
            "unit-backend",
            "unit-frontend", 
            "integration-api",
            "integration-database",
            "e2e-workflows",
            "performance-load",
            "performance-stress",
            "security-auth",
            "security-injection",
            "accessibility",
            "visual-regression"
          ]'
          
          echo "list=$categories" >> $GITHUB_OUTPUT
          echo "Test categories defined successfully"

  # ===== SETUP PHASE =====
  setup-test-environment:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    timeout-minutes: 8
    needs: test-planning
    outputs:
      cache-key: ${{ steps.cache.outputs.key }}
      test-db-ready: ${{ steps.services.outputs.db-ready }}
      mock-services-ready: ${{ steps.services.outputs.mock-ready }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js with enhanced caching
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Generate comprehensive cache keys
        id: cache
        run: |
          # Multi-level cache keys for optimal performance
          base_key="${{ env.CACHE_VERSION }}-${{ runner.os }}-node-${{ env.NODE_VERSION }}"
          package_hash="${{ hashFiles('**/package-lock.json') }}"
          test_config_hash="${{ hashFiles('**/jest.config.*', '**/playwright.config.*', '**/tsconfig.json') }}"
          
          echo "key=$base_key-$package_hash-$test_config_hash" >> $GITHUB_OUTPUT
          echo "deps-key=$base_key-deps-$package_hash" >> $GITHUB_OUTPUT
          echo "build-key=$base_key-build-$package_hash" >> $GITHUB_OUTPUT
          echo "test-cache-key=$base_key-test-cache-$test_config_hash" >> $GITHUB_OUTPUT

      - name: Cache test dependencies and artifacts
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            backend/node_modules
            frontend/node_modules
            ~/.npm
            ~/.cache/playwright
            backend/coverage
            frontend/coverage
            test-results
            .jest-cache
          key: ${{ steps.cache.outputs.key }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-${{ runner.os }}-node-${{ env.NODE_VERSION }}-
            ${{ env.CACHE_VERSION }}-${{ runner.os }}-

      - name: Install system dependencies for testing
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            python3 python3-dev python3-pip \
            make g++ sqlite3 redis-tools postgresql-client \
            chromium-browser firefox-esr \
            xvfb x11-utils

      - name: Install project dependencies
        run: |
          echo "Installing dependencies for comprehensive testing..."
          npm ci --prefer-offline --no-audit
          npm run install:all
          
          # Install Playwright browsers if not cached
          cd frontend && npx playwright install --with-deps chromium firefox webkit

      - name: Setup test services
        id: services
        run: |
          echo "Setting up test services..."
          
          # Start test database
          docker run -d \
            --name test-postgres \
            -e POSTGRES_PASSWORD=testpass \
            -e POSTGRES_USER=testuser \
            -e POSTGRES_DB=testdb \
            -p 5432:5432 \
            postgres:15-alpine
          
          # Start Redis for caching tests
          docker run -d \
            --name test-redis \
            -p 6379:6379 \
            redis:7-alpine
          
          # Wait for services
          echo "Waiting for services to be ready..."
          timeout 60 bash -c 'until pg_isready -h localhost -p 5432 -U testuser; do sleep 2; done'
          timeout 60 bash -c 'until redis-cli -h localhost -p 6379 ping | grep -q PONG; do sleep 2; done'
          
          echo "db-ready=true" >> $GITHUB_OUTPUT
          echo "mock-ready=true" >> $GITHUB_OUTPUT
          echo "✅ Test services ready"

      - name: Initialize test databases
        run: |
          echo "Initializing test databases..."
          
          # Create test schemas
          PGPASSWORD=testpass psql -h localhost -U testuser -d testdb -c "
            CREATE SCHEMA IF NOT EXISTS test_schema;
            CREATE TABLE IF NOT EXISTS test_schema.health_check (
              id SERIAL PRIMARY KEY,
              status VARCHAR(20) DEFAULT 'ok',
              timestamp TIMESTAMP DEFAULT NOW()
            );
            INSERT INTO test_schema.health_check (status) VALUES ('initialized');
          "
          
          echo "✅ Test databases initialized"

      - name: Validate test environment
        run: |
          echo "Validating comprehensive test environment..."
          
          # Check Node.js and npm
          node --version
          npm --version
          
          # Check browsers for E2E testing
          chromium-browser --version || google-chrome --version || echo "Chrome not available"
          firefox --version || echo "Firefox not available"
          
          # Check database connectivity
          pg_isready -h localhost -p 5432 -U testuser
          redis-cli -h localhost -p 6379 ping
          
          # Check test frameworks
          cd backend && npx jest --version
          cd ../frontend && npx jest --version && npx playwright --version
          
          echo "✅ Test environment validation complete"

  # ===== LONDON TDD ENFORCEMENT =====
  london-tdd-validation:
    name: London TDD Compliance Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [test-planning, setup-test-environment]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Restore dependencies
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            backend/node_modules
            frontend/node_modules
          key: ${{ needs.setup-test-environment.outputs.cache-key }}

      - name: Validate London TDD compliance
        run: |
          echo "🧪 LONDON TDD COMPLIANCE VALIDATION"
          echo "════════════════════════════════════════════════════════════"
          
          # Check for test structure compliance
          echo "Checking test structure..."
          
          # Backend test structure validation
          if [[ -d "backend/src/tests" ]]; then
            echo "✅ Backend test directory exists"
            
            # Check for required test categories
            test_categories=("unit" "integration" "security" "performance")
            for category in "${test_categories[@]}"; do
              if [[ -d "backend/src/tests/$category" ]]; then
                echo "✅ $category tests directory found"
              else
                echo "⚠️  $category tests directory missing"
              fi
            done
          else
            echo "❌ Backend test directory missing"
            exit 1
          fi
          
          # Frontend test structure validation
          if [[ -d "frontend/src/__tests__" ]]; then
            echo "✅ Frontend test directory exists"
          else
            echo "❌ Frontend test directory missing"
            exit 1
          fi
          
          echo "✅ London TDD structure validation passed"

      - name: Check test coverage configuration
        run: |
          echo "Checking test coverage configuration..."
          
          # Backend coverage config
          if [[ -f "backend/jest.config.js" ]]; then
            echo "✅ Backend Jest configuration exists"
            
            # Check coverage thresholds
            if grep -q "coverageThreshold" backend/jest.config.js; then
              echo "✅ Coverage thresholds configured"
            else
              echo "⚠️  Coverage thresholds not configured"
            fi
          fi
          
          # Frontend coverage config
          if [[ -f "frontend/jest.config.js" ]]; then
            echo "✅ Frontend Jest configuration exists"
          fi
          
          echo "✅ Coverage configuration validation passed"

      - name: Validate test naming conventions
        run: |
          echo "Validating London TDD test naming conventions..."
          
          # Check for proper test file naming
          invalid_tests=0
          
          # Backend test naming
          find backend/src/tests -name "*.test.ts" -o -name "*.spec.ts" | while read -r test_file; do
            if [[ ! "$test_file" =~ \.(test|spec)\.ts$ ]]; then
              echo "❌ Invalid test file naming: $test_file"
              ((invalid_tests++))
            fi
          done
          
          # Frontend test naming
          find frontend/src/__tests__ -name "*.test.ts*" -o -name "*.spec.ts*" | while read -r test_file; do
            if [[ ! "$test_file" =~ \.(test|spec)\.(ts|tsx)$ ]]; then
              echo "❌ Invalid test file naming: $test_file"
              ((invalid_tests++))
            fi
          done
          
          if [[ $invalid_tests -eq 0 ]]; then
            echo "✅ Test naming conventions validation passed"
          else
            echo "❌ $invalid_tests test files have invalid naming"
            exit 1
          fi

  # ===== PARALLEL TEST EXECUTION =====
  unit-tests:
    name: Unit Tests (${{ matrix.test-type }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: ${{ matrix.timeout }}
    needs: [test-planning, setup-test-environment, london-tdd-validation]
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.test-planning.outputs.test-matrix) }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Restore test environment
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            backend/node_modules
            frontend/node_modules
            ~/.cache/playwright
          key: ${{ needs.setup-test-environment.outputs.cache-key }}

      - name: Start test services
        if: matrix.test-type == 'integration'
        run: |
          docker run -d --name test-postgres \
            -e POSTGRES_PASSWORD=testpass \
            -e POSTGRES_USER=testuser \
            -e POSTGRES_DB=testdb \
            -p 5432:5432 postgres:15-alpine
          
          docker run -d --name test-redis \
            -p 6379:6379 redis:7-alpine
          
          # Wait for services
          timeout 60 bash -c 'until pg_isready -h localhost -p 5432 -U testuser; do sleep 2; done'
          timeout 60 bash -c 'until redis-cli -h localhost -p 6379 ping | grep -q PONG; do sleep 2; done'

      - name: Run backend unit tests
        if: contains(matrix.test-type, 'unit') && matrix.test-type != 'unit-frontend'
        run: |
          echo "🧪 Running backend unit tests with London TDD enforcement..."
          cd backend
          
          # Configure test environment
          export NODE_ENV=test
          export CI=true
          export COVERAGE_THRESHOLD=${{ needs.test-planning.outputs.coverage-threshold }}
          
          # Run tests with coverage enforcement
          npm run test:unit -- \
            --coverage \
            --coverageReporters=text-lcov \
            --coverageReporters=html \
            --coverageThreshold='{"global":{"lines":$COVERAGE_THRESHOLD,"functions":$COVERAGE_THRESHOLD,"branches":$COVERAGE_THRESHOLD,"statements":$COVERAGE_THRESHOLD}}' \
            --testTimeout=10000 \
            --maxWorkers=${{ env.JEST_WORKERS }} \
            --passWithNoTests=false
        env:
          NODE_ENV: test
          CI: true

      - name: Run frontend unit tests
        if: contains(matrix.test-type, 'unit') && matrix.test-type != 'unit-backend'
        run: |
          echo "🧪 Running frontend unit tests with London TDD enforcement..."
          cd frontend
          
          # Configure test environment
          export NODE_ENV=test
          export CI=true
          export COVERAGE_THRESHOLD=${{ needs.test-planning.outputs.coverage-threshold }}
          
          # Run tests with coverage enforcement
          npm run test:unit -- \
            --coverage \
            --coverageReporters=text-lcov \
            --coverageReporters=html \
            --coverageThreshold='{"global":{"lines":$COVERAGE_THRESHOLD,"functions":$COVERAGE_THRESHOLD,"branches":$COVERAGE_THRESHOLD,"statements":$COVERAGE_THRESHOLD}}' \
            --testTimeout=10000 \
            --maxWorkers=${{ env.JEST_WORKERS }} \
            --passWithNoTests=false
        env:
          NODE_ENV: test
          CI: true

      - name: Run integration tests
        if: matrix.test-type == 'integration'
        run: |
          echo "🔗 Running integration tests..."
          
          # Backend integration tests
          cd backend
          npm run test:integration -- \
            --coverage \
            --testTimeout=15000 \
            --maxWorkers=50% \
            --runInBand=false
          
          # Frontend integration tests
          cd ../frontend  
          npm run test:integration -- \
            --testTimeout=15000 \
            --maxWorkers=50%
        env:
          NODE_ENV: test
          CI: true
          DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
          REDIS_URL: redis://localhost:6379

      - name: Run E2E tests
        if: matrix.test-type == 'e2e'
        run: |
          echo "🎭 Running E2E tests..."
          cd frontend
          
          # Start backend for E2E testing
          cd ../backend && npm run build && npm start &
          backend_pid=$!
          
          # Wait for backend to be ready
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'
          
          # Run E2E tests
          cd ../frontend
          npm run test:e2e:ci
          
          # Cleanup
          kill $backend_pid || true
        env:
          NODE_ENV: test
          CI: true

      - name: Run performance tests
        if: matrix.test-type == 'performance'
        run: |
          echo "⚡ Running performance tests..."
          cd backend
          
          # Performance benchmarks
          npm run test:performance -- \
            --testTimeout=30000 \
            --runInBand=true
        env:
          NODE_ENV: test
          CI: true

      - name: Run security tests
        if: matrix.test-type == 'security'
        run: |
          echo "🔒 Running security tests..."
          
          # Backend security tests
          cd backend
          npm run test -- --testNamePattern="security|auth|validation" \
            --testTimeout=15000
          
          # Frontend security tests
          cd ../frontend
          npm run test -- --testNamePattern="security|auth|validation" \
            --testTimeout=15000
        env:
          NODE_ENV: test
          CI: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-type }}-${{ github.run_id }}
          path: |
            backend/coverage/
            frontend/coverage/
            backend/test-results/
            frontend/test-results/
            playwright-report/
          retention-days: 7

      - name: Upload coverage to Codecov
        if: matrix.coverage-required == true
        uses: codecov/codecov-action@v4
        with:
          files: |
            ./backend/coverage/lcov.info
            ./frontend/coverage/lcov.info
          flags: ${{ matrix.test-type }}
          name: ${{ matrix.test-type }}-coverage
          fail_ci_if_error: false

  # ===== MUTATION TESTING =====
  mutation-testing:
    name: Mutation Testing (Quality Assurance)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test-planning, unit-tests]
    if: needs.test-planning.outputs.should-run-mutation == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install Stryker (Mutation Testing)
        run: |
          npm install -g @stryker-mutator/core @stryker-mutator/jest-runner @stryker-mutator/typescript-checker

      - name: Run mutation testing
        run: |
          echo "🧬 Running mutation testing to validate test quality..."
          
          # Backend mutation testing
          cd backend
          stryker run --testRunner jest --coverageAnalysis perTest
          
          echo "✅ Mutation testing completed"

      - name: Upload mutation test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mutation-test-results-${{ github.run_id }}
          path: |
            backend/reports/mutation/
            frontend/reports/mutation/
          retention-days: 7

  # ===== VISUAL REGRESSION TESTING =====
  visual-regression:
    name: Visual Regression Testing
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [test-planning, setup-test-environment]
    if: contains(github.event.head_commit.message, '[visual]') || github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Restore test environment
        uses: actions/cache@v4
        with:
          path: |
            frontend/node_modules
            ~/.cache/playwright
          key: ${{ needs.setup-test-environment.outputs.cache-key }}

      - name: Run visual regression tests
        run: |
          echo "👀 Running visual regression tests..."
          cd frontend
          
          # Start application for visual testing
          npm run build
          npm start &
          app_pid=$!
          
          # Wait for app to be ready
          timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done'
          
          # Run visual tests
          npm run test:visual
          
          # Cleanup
          kill $app_pid || true

      - name: Upload visual test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: visual-regression-results-${{ github.run_id }}
          path: |
            frontend/test-results/
            frontend/visual-report/
          retention-days: 7

  # ===== COMPREHENSIVE QUALITY ASSESSMENT =====
  quality-assessment:
    name: Quality Assessment & Reporting
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test-planning, unit-tests, london-tdd-validation]
    if: always()
    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: '*-${{ github.run_id }}'
          merge-multiple: true
          path: ./test-artifacts/

      - name: Generate comprehensive quality report
        run: |
          echo "📊 COMPREHENSIVE QUALITY ASSESSMENT REPORT"
          echo "═══════════════════════════════════════════════════════════════"
          
          # Calculate test results
          total_jobs=$(echo '${{ toJson(needs) }}' | jq '[.[] | select(.result != null)] | length')
          successful_jobs=$(echo '${{ toJson(needs) }}' | jq '[.[] | select(.result == "success")] | length')
          failed_jobs=$(echo '${{ toJson(needs) }}' | jq '[.[] | select(.result == "failure")] | length')
          skipped_jobs=$(echo '${{ toJson(needs) }}' | jq '[.[] | select(.result == "skipped")] | length')
          
          success_rate=$((successful_jobs * 100 / total_jobs))
          
          echo "📊 Test Execution Summary:"
          echo "├── Total Test Jobs: $total_jobs"
          echo "├── Successful: $successful_jobs ✅"
          echo "├── Failed: $failed_jobs ❌"
          echo "├── Skipped: $skipped_jobs ⏭️"
          echo "└── Success Rate: $success_rate%"
          echo ""
          
          # London TDD Compliance
          tdd_compliance="${{ needs.london-tdd-validation.result }}"
          echo "🧪 London TDD Compliance:"
          echo "├── TDD Structure: $([ "$tdd_compliance" == "success" ] && echo "✅ PASSED" || echo "❌ FAILED")"
          echo "├── Coverage Threshold: ${{ needs.test-planning.outputs.coverage-threshold }}%"
          echo "└── Test Level: ${{ env.TEST_LEVEL }}"
          echo ""
          
          # Quality Gate Assessment
          if [[ $success_rate -ge 95 && "$tdd_compliance" == "success" ]]; then
            echo "🏆 QUALITY GATE: ✅ PASSED (EXCELLENT)"
            echo "All quality requirements met with exemplary standards"
            exit_code=0
          elif [[ $success_rate -ge 85 && "$tdd_compliance" == "success" ]]; then
            echo "✅ QUALITY GATE: ✅ PASSED (GOOD)"
            echo "Quality requirements met with minor issues"
            exit_code=0
          elif [[ $success_rate -ge 70 ]]; then
            echo "⚠️  QUALITY GATE: ⚠️  CONDITIONAL PASS"
            echo "Quality requirements partially met - review required"
            exit_code=0
          else
            echo "❌ QUALITY GATE: ❌ FAILED"
            echo "Quality requirements not met - improvements required"
            exit_code=1
          fi
          
          echo ""
          echo "📋 Detailed Results:"
          echo '${{ toJson(needs) }}' | jq -r 'to_entries[] | "├── \(.key): \(.value.result)"'
          
          exit $exit_code

      - name: Create test summary
        if: always()
        run: |
          {
            echo "# 🧪 Test Execution Summary"
            echo ""
            echo "## 📊 Results Overview"
            echo "- **Test Level**: ${{ env.TEST_LEVEL }}"
            echo "- **Coverage Threshold**: ${{ needs.test-planning.outputs.coverage-threshold }}%"
            echo "- **London TDD**: ${{ needs.london-tdd-validation.result == 'success' && '✅ Compliant' || '❌ Non-compliant' }}"
            echo ""
            echo "## 🎯 Quality Gate Status"
            echo "**Status**: ${{ job.status == 'success' && '✅ PASSED' || '❌ FAILED' }}"
            echo ""
            echo "## 📁 Test Artifacts"
            echo "Test results and coverage reports are available in the job artifacts."
          } >> $GITHUB_STEP_SUMMARY

      - name: Post quality metrics
        if: always()
        continue-on-error: true
        run: |
          # Send quality metrics to monitoring system
          curl -X POST -H "Content-Type: application/json" \
            -d '{
              "test_execution": {
                "id": "${{ github.run_id }}",
                "workflow": "${{ github.workflow }}",
                "branch": "${{ github.ref_name }}",
                "commit": "${{ github.sha }}",
                "test_level": "${{ env.TEST_LEVEL }}",
                "coverage_threshold": ${{ needs.test-planning.outputs.coverage-threshold }},
                "tdd_compliance": "${{ needs.london-tdd-validation.result }}",
                "quality_gate": "${{ job.status }}",
                "timestamp": "${{ github.event.repository.updated_at }}"
              }
            }' \
            http://localhost:3001/api/quality/metrics || echo "Failed to post metrics"

  # ===== PERFORMANCE BENCHMARKING =====
  performance-benchmarking:
    name: Performance Benchmarking & Regression Detection
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test-planning, setup-test-environment]
    if: github.ref == 'refs/heads/main' || contains(github.event.head_commit.message, '[perf]')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install performance testing tools
        run: |
          npm install -g clinic autocannon
          npm install --save-dev benchmark

      - name: Run performance benchmarks
        run: |
          echo "⚡ Running comprehensive performance benchmarks..."
          
          # Backend performance benchmarks
          cd backend
          npm run build
          npm start &
          backend_pid=$!
          
          # Wait for backend
          timeout 60 bash -c 'until curl -f http://localhost:3001/api/health; do sleep 2; done'
          
          # Run load tests
          autocannon -c 10 -d 30 -p 10 http://localhost:3001/api/health > performance-results.txt
          
          # Performance profiling
          clinic doctor --on-port 'autocannon -c 5 -d 10 http://localhost:3001/api/health' -- node dist/server.js &
          
          # Frontend performance
          cd ../frontend
          npm run build
          
          # Lighthouse performance audit
          npm install -g @lhci/cli
          lhci autorun || echo "Lighthouse CI not configured"
          
          # Cleanup
          kill $backend_pid || true

      - name: Analyze performance regression
        run: |
          echo "📈 Analyzing performance regression..."
          
          # Compare with baseline if available
          if [[ -f "performance-baseline.json" ]]; then
            echo "Comparing with performance baseline..."
            # Performance regression detection logic would go here
          else
            echo "No baseline found, creating new baseline..."
            # Create baseline for future comparisons
          fi

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.run_id }}
          path: |
            backend/performance-results.txt
            backend/.clinic/
            frontend/lhci_reports/
          retention-days: 14