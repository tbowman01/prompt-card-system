{"version":3,"sources":["/workspaces/prompt-card-system/backend/src/services/training/ModelTrainingEngine.ts"],"sourcesContent":["import { EventEmitter } from 'events';\nimport { llmService } from '../llmService';\nimport { ModelHealthMonitor } from '../models/ModelHealthMonitor';\nimport { OptimizationEngine } from '../optimization/OptimizationEngine';\nimport { EventStore } from '../analytics/EventStore';\nimport { LRUCache } from 'lru-cache';\nimport { performance } from 'perf_hooks';\nimport * as tf from '@tensorflow/tfjs-node';\nimport { createHash } from 'crypto';\nimport axios from 'axios';\n\nexport interface TrainingConfiguration {\n  id: string;\n  name: string;\n  model: string;\n  trainingData: {\n    source: 'file' | 'database' | 'api' | 'synthetic';\n    path?: string;\n    query?: string;\n    endpoint?: string;\n    format: 'jsonl' | 'csv' | 'txt' | 'parquet';\n    validation_split: number;\n  };\n  hyperparameters: {\n    learning_rate: number;\n    batch_size: number;\n    epochs: number;\n    warmup_steps: number;\n    weight_decay: number;\n    dropout_rate: number;\n    gradient_clip_norm: number;\n  };\n  optimization: {\n    optimizer: 'adam' | 'adamw' | 'sgd' | 'rmsprop';\n    scheduler: 'linear' | 'cosine' | 'exponential' | 'polynomial';\n    early_stopping: {\n      enabled: boolean;\n      patience: number;\n      metric: string;\n      min_delta: number;\n    };\n  };\n  evaluation: {\n    metrics: string[];\n    benchmark_datasets: string[];\n    validation_frequency: number;\n    save_best_model: boolean;\n  };\n  resources: {\n    gpu_memory_limit?: number;\n    cpu_cores?: number;\n    memory_limit?: number;\n    storage_limit?: number;\n  };\n  deployment: {\n    auto_deploy: boolean;\n    deployment_target: 'ollama' | 'huggingface' | 'local' | 'cloud';\n    rollback_on_failure: boolean;\n    health_check_enabled: boolean;\n  };\n  metadata: {\n    created_by: string;\n    created_at: Date;\n    tags: string[];\n    description: string;\n    base_model?: string;\n    training_objective: string;\n  };\n}\n\nexport interface TrainingJob {\n  id: string;\n  config: TrainingConfiguration;\n  status: 'pending' | 'initializing' | 'training' | 'evaluating' | 'completed' | 'failed' | 'cancelled';\n  progress: {\n    current_epoch: number;\n    total_epochs: number;\n    current_step: number;\n    total_steps: number;\n    elapsed_time: number;\n    estimated_remaining: number;\n    train_loss: number;\n    validation_loss: number;\n    best_metric: number;\n  };\n  metrics: {\n    training_loss: number[];\n    validation_loss: number[];\n    learning_rate: number[];\n    custom_metrics: Record<string, number[]>;\n  };\n  logs: TrainingLog[];\n  artifacts: {\n    model_path?: string;\n    checkpoints: string[];\n    evaluation_reports: string[];\n    tensorboard_logs?: string;\n  };\n  error?: {\n    message: string;\n    stack?: string;\n    timestamp: Date;\n  };\n  started_at?: Date;\n  completed_at?: Date;\n}\n\nexport interface TrainingLog {\n  timestamp: Date;\n  level: 'info' | 'warning' | 'error' | 'debug';\n  message: string;\n  metadata?: Record<string, any>;\n}\n\nexport interface TrainingModelVersion {\n  id: string;\n  model_name: string;\n  version: string;\n  base_model: string;\n  training_job_id: string;\n  performance_metrics: {\n    accuracy: number;\n    f1_score: number;\n    perplexity: number;\n    inference_time: number;\n    memory_usage: number;\n    throughput: number;\n  };\n  model_size: number;\n  deployment_status: 'pending' | 'deployed' | 'deprecated' | 'failed';\n  created_at: Date;\n  deployed_at?: Date;\n  deprecated_at?: Date;\n}\n\nexport interface SyntheticDataGeneration {\n  id: string;\n  template_prompts: string[];\n  generation_config: {\n    num_samples: number;\n    temperature: number;\n    max_tokens: number;\n    diversity_penalty: number;\n    quality_filter: boolean;\n  };\n  output_format: 'jsonl' | 'csv' | 'txt';\n  quality_metrics: {\n    coherence_score: number;\n    relevance_score: number;\n    diversity_score: number;\n    safety_score: number;\n  };\n  status: 'pending' | 'generating' | 'completed' | 'failed';\n  progress: {\n    generated_samples: number;\n    total_samples: number;\n    current_template: number;\n    total_templates: number;\n  };\n}\n\nexport class ModelTrainingEngine extends EventEmitter {\n  private eventStore: EventStore;\n  private modelHealthMonitor: ModelHealthMonitor;\n  private optimizationEngine: OptimizationEngine;\n  private activeJobs: Map<string, TrainingJob>;\n  private modelRegistry: Map<string, TrainingModelVersion[]>;\n  private trainingCache: LRUCache<string, any>;\n  private performanceMetrics: Map<string, number[]>;\n  private isInitialized = false;\n\n  constructor() {\n    super();\n    this.eventStore = EventStore.getInstance();\n    this.activeJobs = new Map();\n    this.modelRegistry = new Map();\n    \n    // Initialize services\n    this.modelHealthMonitor = new ModelHealthMonitor({\n      healthCheckInterval: 60000,\n      benchmarkInterval: 300000,\n      maxResponseTime: 30000,\n      maxErrorRate: 10,\n      minHealthScore: 70,\n      alertThresholds: {\n        responseTime: 15000,\n        errorRate: 10,\n        memoryUsage: 85\n      }\n    });\n    this.optimizationEngine = new OptimizationEngine();\n    \n    // Initialize caches\n    this.trainingCache = new LRUCache({\n      max: 100,\n      ttl: 1000 * 60 * 60 * 2 // 2 hours\n    });\n    \n    this.performanceMetrics = new Map();\n  }\n\n  async initialize(): Promise<void> {\n    if (this.isInitialized) return;\n\n    try {\n      console.log('üöÄ Initializing Model Training Engine...');\n      \n      // Initialize TensorFlow backend\n      await this.initializeTensorFlow();\n      \n      // Load existing model registry\n      await this.loadModelRegistry();\n      \n      // Resume any interrupted training jobs\n      await this.resumeInterruptedJobs();\n      \n      this.isInitialized = true;\n      console.log('‚úÖ Model Training Engine initialized successfully');\n      \n      this.emit('initialized', { timestamp: new Date() });\n    } catch (error) {\n      console.error('‚ùå Failed to initialize Model Training Engine:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Create a new training job\n   */\n  async createTrainingJob(config: Omit<TrainingConfiguration, 'id' | 'metadata'>): Promise<TrainingJob> {\n    const jobId = `train_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    const fullConfig: TrainingConfiguration = {\n      ...config,\n      id: jobId,\n      metadata: {\n        created_by: 'system',\n        created_at: new Date(),\n        tags: [],\n        description: '',\n        base_model: config.model,\n        training_objective: 'general_improvement'\n      }\n    };\n\n    // Validate configuration\n    await this.validateTrainingConfig(fullConfig);\n\n    const job: TrainingJob = {\n      id: jobId,\n      config: fullConfig,\n      status: 'pending',\n      progress: {\n        current_epoch: 0,\n        total_epochs: config.hyperparameters.epochs,\n        current_step: 0,\n        total_steps: 0,\n        elapsed_time: 0,\n        estimated_remaining: 0,\n        train_loss: 0,\n        validation_loss: 0,\n        best_metric: 0\n      },\n      metrics: {\n        training_loss: [],\n        validation_loss: [],\n        learning_rate: [],\n        custom_metrics: {}\n      },\n      logs: [],\n      artifacts: {\n        checkpoints: [],\n        evaluation_reports: []\n      }\n    };\n\n    this.activeJobs.set(jobId, job);\n\n    // Record job creation\n    await this.eventStore.recordEvent({\n      event_type: 'training_job_created',\n      entity_id: jobId,\n      entity_type: 'training_job',\n      data: { config: fullConfig },\n      timestamp: new Date()\n    });\n\n    this.emit('jobCreated', { jobId, config: fullConfig });\n\n    return job;\n  }\n\n  /**\n   * Start a training job\n   */\n  async startTrainingJob(jobId: string): Promise<void> {\n    const job = this.activeJobs.get(jobId);\n    if (!job) {\n      throw new Error(`Training job ${jobId} not found`);\n    }\n\n    if (job.status !== 'pending') {\n      throw new Error(`Training job ${jobId} is not in pending status`);\n    }\n\n    job.status = 'initializing';\n    job.started_at = new Date();\n\n    this.addTrainingLog(job, 'info', 'Training job started');\n\n    // Start training in background\n    this.executeTrainingJob(job).catch(error => {\n      this.handleTrainingError(job, error);\n    });\n\n    await this.eventStore.recordEvent({\n      event_type: 'training_job_started',\n      entity_id: jobId,\n      entity_type: 'training_job',\n      data: { status: job.status },\n      timestamp: new Date()\n    });\n\n    this.emit('jobStarted', { jobId, status: job.status });\n  }\n\n  /**\n   * Generate synthetic training data\n   */\n  async generateSyntheticData(config: SyntheticDataGeneration): Promise<string> {\n    const generationId = `synth_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    try {\n      config.status = 'generating';\n      config.progress = {\n        generated_samples: 0,\n        total_samples: config.generation_config.num_samples,\n        current_template: 0,\n        total_templates: config.template_prompts.length\n      };\n\n      const generatedData: any[] = [];\n      \n      for (let templateIndex = 0; templateIndex < config.template_prompts.length; templateIndex++) {\n        const template = config.template_prompts[templateIndex];\n        config.progress.current_template = templateIndex;\n        \n        const samplesPerTemplate = Math.ceil(config.generation_config.num_samples / config.template_prompts.length);\n        \n        for (let i = 0; i < samplesPerTemplate && config.progress.generated_samples < config.generation_config.num_samples; i++) {\n          try {\n            // Generate variation of the template\n            const prompt = await this.generatePromptVariation(template, config.generation_config);\n            \n            // Generate response using LLM\n            const response = await llmService.generate(prompt, undefined, {\n              temperature: config.generation_config.temperature,\n              num_predict: config.generation_config.max_tokens\n            });\n\n            // Apply quality filtering if enabled\n            if (config.generation_config.quality_filter) {\n              const qualityScore = await this.assessDataQuality(prompt, response.response);\n              if (qualityScore < 0.7) continue; // Skip low-quality samples\n            }\n\n            generatedData.push({\n              prompt: prompt,\n              response: response.response,\n              template_id: templateIndex,\n              generation_id: generationId,\n              timestamp: new Date()\n            });\n\n            config.progress.generated_samples++;\n          } catch (error) {\n            console.warn(`Failed to generate sample ${i} for template ${templateIndex}:`, error);\n          }\n        }\n      }\n\n      // Calculate quality metrics\n      config.quality_metrics = await this.calculateDatasetQuality(generatedData);\n      \n      // Save generated data\n      const outputPath = `/tmp/synthetic_data_${generationId}.${config.output_format}`;\n      await this.saveGeneratedData(generatedData, outputPath, config.output_format);\n      \n      config.status = 'completed';\n      \n      await this.eventStore.recordEvent({\n        event_type: 'synthetic_data_generated',\n        entity_id: generationId,\n        entity_type: 'synthetic_data',\n        data: {\n          config,\n          output_path: outputPath,\n          samples_generated: generatedData.length\n        },\n        timestamp: new Date()\n      });\n\n      return outputPath;\n    } catch (error) {\n      config.status = 'failed';\n      console.error('Failed to generate synthetic data:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Evaluate model performance\n   */\n  async evaluateModel(\n    modelName: string,\n    benchmarkDatasets: string[],\n    customMetrics: string[] = []\n  ): Promise<{\n    overall_score: number;\n    detailed_metrics: Record<string, number>;\n    benchmark_results: Record<string, any>;\n    recommendations: string[];\n  }> {\n    const evaluationId = `eval_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    try {\n      console.log(`üîç Evaluating model: ${modelName}`);\n      \n      const benchmarkResults: Record<string, any> = {};\n      const detailedMetrics: Record<string, number> = {};\n      \n      // Run benchmark evaluations\n      for (const dataset of benchmarkDatasets) {\n        const result = await this.runBenchmarkEvaluation(modelName, dataset);\n        benchmarkResults[dataset] = result;\n        \n        // Aggregate metrics\n        Object.entries(result.metrics).forEach(([metric, value]) => {\n          if (!detailedMetrics[metric]) detailedMetrics[metric] = 0;\n          detailedMetrics[metric] += value as number;\n        });\n      }\n      \n      // Average metrics across datasets\n      Object.keys(detailedMetrics).forEach(metric => {\n        detailedMetrics[metric] /= benchmarkDatasets.length;\n      });\n      \n      // Run custom metrics\n      for (const metric of customMetrics) {\n        const value = await this.calculateCustomMetric(modelName, metric);\n        detailedMetrics[metric] = value;\n      }\n      \n      // Calculate overall score\n      const overallScore = this.calculateOverallScore(detailedMetrics);\n      \n      // Generate recommendations\n      const recommendations = await this.generatePerformanceRecommendations(\n        modelName,\n        detailedMetrics,\n        benchmarkResults\n      );\n      \n      const evaluationResult = {\n        overall_score: overallScore,\n        detailed_metrics: detailedMetrics,\n        benchmark_results: benchmarkResults,\n        recommendations\n      };\n      \n      // Store evaluation results\n      await this.eventStore.recordEvent({\n        event_type: 'model_evaluation',\n        entity_id: evaluationId,\n        entity_type: 'evaluation',\n        data: {\n          model_name: modelName,\n          ...evaluationResult\n        },\n        timestamp: new Date()\n      });\n      \n      return evaluationResult;\n    } catch (error) {\n      console.error(`Failed to evaluate model ${modelName}:`, error);\n      throw error;\n    }\n  }\n\n  /**\n   * Deploy trained model\n   */\n  async deployModel(\n    modelVersionId: string,\n    target: 'ollama' | 'huggingface' | 'local' | 'cloud',\n    config: {\n      auto_rollback?: boolean;\n      health_check_timeout?: number;\n      deployment_tags?: string[];\n    } = {}\n  ): Promise<{\n    deployment_id: string;\n    status: 'success' | 'failed';\n    endpoint?: string;\n    health_status?: any;\n  }> {\n    const deploymentId = `deploy_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    try {\n      console.log(`üöÄ Deploying model version: ${modelVersionId} to ${target}`);\n      \n      // Get model version details\n      const modelVersion = await this.getModelVersion(modelVersionId);\n      if (!modelVersion) {\n        throw new Error(`Model version ${modelVersionId} not found`);\n      }\n      \n      let deploymentResult;\n      \n      switch (target) {\n        case 'ollama':\n          deploymentResult = await this.deployToOllama(modelVersion, config);\n          break;\n        case 'huggingface':\n          deploymentResult = await this.deployToHuggingFace(modelVersion, config);\n          break;\n        case 'local':\n          deploymentResult = await this.deployToLocal(modelVersion, config);\n          break;\n        case 'cloud':\n          deploymentResult = await this.deployToCloud(modelVersion, config);\n          break;\n        default:\n          throw new Error(`Unsupported deployment target: ${target}`);\n      }\n      \n      // Update model version status\n      modelVersion.deployment_status = deploymentResult.status === 'success' ? 'deployed' : 'failed';\n      if (deploymentResult.status === 'success') {\n        modelVersion.deployed_at = new Date();\n      }\n      \n      await this.updateModelVersion(modelVersion);\n      \n      // Record deployment\n      await this.eventStore.recordEvent({\n        event_type: 'model_deployed',\n        entity_id: deploymentId,\n        entity_type: 'deployment',\n        data: {\n          model_version_id: modelVersionId,\n          target,\n          config,\n          result: deploymentResult\n        },\n        timestamp: new Date()\n      });\n      \n      return {\n        deployment_id: deploymentId,\n        ...deploymentResult\n      };\n    } catch (error) {\n      console.error(`Failed to deploy model ${modelVersionId}:`, error);\n      \n      await this.eventStore.recordEvent({\n        event_type: 'model_deployment_failed',\n        entity_id: deploymentId,\n        entity_type: 'deployment',\n        data: {\n          model_version_id: modelVersionId,\n          target,\n          error: error instanceof Error ? error.message : 'Unknown error'\n        },\n        timestamp: new Date()\n      });\n      \n      throw error;\n    }\n  }\n\n  /**\n   * Get training job status\n   */\n  getTrainingJob(jobId: string): TrainingJob | undefined {\n    return this.activeJobs.get(jobId);\n  }\n\n  /**\n   * List all training jobs\n   */\n  listTrainingJobs(filters: {\n    status?: string;\n    model?: string;\n    created_after?: Date;\n    created_before?: Date;\n  } = {}): TrainingJob[] {\n    const jobs = Array.from(this.activeJobs.values());\n    \n    return jobs.filter(job => {\n      if (filters.status && job.status !== filters.status) return false;\n      if (filters.model && job.config.model !== filters.model) return false;\n      if (filters.created_after && job.config.metadata.created_at < filters.created_after) return false;\n      if (filters.created_before && job.config.metadata.created_at > filters.created_before) return false;\n      return true;\n    });\n  }\n\n  /**\n   * Cancel training job\n   */\n  async cancelTrainingJob(jobId: string): Promise<void> {\n    const job = this.activeJobs.get(jobId);\n    if (!job) {\n      throw new Error(`Training job ${jobId} not found`);\n    }\n\n    if (!['pending', 'initializing', 'training'].includes(job.status)) {\n      throw new Error(`Training job ${jobId} cannot be cancelled in status: ${job.status}`);\n    }\n\n    job.status = 'cancelled';\n    this.addTrainingLog(job, 'info', 'Training job cancelled by user');\n\n    await this.eventStore.recordEvent({\n      event_type: 'training_job_cancelled',\n      entity_id: jobId,\n      entity_type: 'training_job',\n      data: { status: job.status },\n      timestamp: new Date()\n    });\n\n    this.emit('jobCancelled', { jobId, status: job.status });\n  }\n\n  /**\n   * Get model registry\n   */\n  getModelRegistry(): Map<string, TrainingModelVersion[]> {\n    return new Map(this.modelRegistry);\n  }\n\n  /**\n   * Get training performance statistics\n   */\n  getTrainingStatistics(): {\n    total_jobs: number;\n    successful_jobs: number;\n    failed_jobs: number;\n    average_training_time: number;\n    models_deployed: number;\n    total_training_hours: number;\n  } {\n    const jobs = Array.from(this.activeJobs.values());\n    const completedJobs = jobs.filter(job => job.status === 'completed');\n    const failedJobs = jobs.filter(job => job.status === 'failed');\n    \n    const averageTrainingTime = completedJobs.length > 0\n      ? completedJobs.reduce((sum, job) => sum + job.progress.elapsed_time, 0) / completedJobs.length\n      : 0;\n    \n    const totalTrainingHours = jobs.reduce((sum, job) => sum + job.progress.elapsed_time, 0) / (1000 * 60 * 60);\n    \n    const modelsDeployed = Array.from(this.modelRegistry.values())\n      .flat()\n      .filter(version => version.deployment_status === 'deployed').length;\n\n    return {\n      total_jobs: jobs.length,\n      successful_jobs: completedJobs.length,\n      failed_jobs: failedJobs.length,\n      average_training_time: averageTrainingTime,\n      models_deployed: modelsDeployed,\n      total_training_hours: totalTrainingHours\n    };\n  }\n\n  // Private methods\n  private async initializeTensorFlow(): Promise<void> {\n    try {\n      // Set TensorFlow backend\n      tf.setBackend('tensorflow');\n      console.log('‚úÖ TensorFlow backend initialized');\n    } catch (error) {\n      console.warn('‚ö†Ô∏è Failed to initialize TensorFlow backend, using CPU fallback');\n    }\n  }\n\n  private async loadModelRegistry(): Promise<void> {\n    try {\n      // Load existing model versions from database/storage\n      // This would connect to your actual storage system\n      console.log('üìã Model registry loaded');\n    } catch (error) {\n      console.warn('‚ö†Ô∏è Failed to load model registry:', error);\n    }\n  }\n\n  private async resumeInterruptedJobs(): Promise<void> {\n    try {\n      // Resume any training jobs that were interrupted\n      console.log('üîÑ Checking for interrupted training jobs...');\n    } catch (error) {\n      console.warn('‚ö†Ô∏è Failed to resume interrupted jobs:', error);\n    }\n  }\n\n  private async validateTrainingConfig(config: TrainingConfiguration): Promise<void> {\n    // Validate training configuration\n    if (!config.model) {\n      throw new Error('Model name is required');\n    }\n    \n    if (config.hyperparameters.epochs <= 0) {\n      throw new Error('Epochs must be greater than 0');\n    }\n    \n    if (config.hyperparameters.learning_rate <= 0 || config.hyperparameters.learning_rate > 1) {\n      throw new Error('Learning rate must be between 0 and 1');\n    }\n    \n    if (config.trainingData.validation_split <= 0 || config.trainingData.validation_split >= 1) {\n      throw new Error('Validation split must be between 0 and 1');\n    }\n  }\n\n  private async executeTrainingJob(job: TrainingJob): Promise<void> {\n    try {\n      job.status = 'training';\n      this.addTrainingLog(job, 'info', 'Starting training process');\n      \n      // Simulate training process (replace with actual training logic)\n      await this.simulateTraining(job);\n      \n      job.status = 'evaluating';\n      this.addTrainingLog(job, 'info', 'Training completed, starting evaluation');\n      \n      // Evaluate trained model\n      const evaluation = await this.evaluateTrainedModel(job);\n      \n      // Create model version\n      const modelVersion = await this.createModelVersion(job, evaluation);\n      \n      job.status = 'completed';\n      job.completed_at = new Date();\n      \n      this.addTrainingLog(job, 'info', `Training completed successfully. Model version: ${modelVersion.id}`);\n      \n      await this.eventStore.recordEvent({\n        event_type: 'training_job_completed',\n        entity_id: job.id,\n        entity_type: 'training_job',\n        data: {\n          model_version_id: modelVersion.id,\n          performance_metrics: modelVersion.performance_metrics\n        },\n        timestamp: new Date()\n      });\n      \n      this.emit('jobCompleted', {\n        jobId: job.id,\n        modelVersionId: modelVersion.id,\n        metrics: modelVersion.performance_metrics\n      });\n      \n    } catch (error) {\n      this.handleTrainingError(job, error);\n    }\n  }\n\n  private async simulateTraining(job: TrainingJob): Promise<void> {\n    // This is a simulation - replace with actual training logic\n    const totalSteps = job.config.hyperparameters.epochs * 100; // Assume 100 steps per epoch\n    job.progress.total_steps = totalSteps;\n    \n    for (let epoch = 1; epoch <= job.config.hyperparameters.epochs; epoch++) {\n      job.progress.current_epoch = epoch;\n      \n      for (let step = 1; step <= 100; step++) {\n        if (job.status === 'cancelled') {\n          throw new Error('Training cancelled by user');\n        }\n        \n        job.progress.current_step = (epoch - 1) * 100 + step;\n        \n        // Simulate training metrics\n        const trainLoss = Math.max(0.1, 2.0 * Math.exp(-job.progress.current_step / 1000) + Math.random() * 0.1);\n        const valLoss = trainLoss * (1.1 + Math.random() * 0.2);\n        const lr = job.config.hyperparameters.learning_rate * Math.pow(0.95, epoch - 1);\n        \n        job.progress.train_loss = trainLoss;\n        job.progress.validation_loss = valLoss;\n        job.metrics.training_loss.push(trainLoss);\n        job.metrics.validation_loss.push(valLoss);\n        job.metrics.learning_rate.push(lr);\n        \n        // Update best metric\n        if (step === 1 && epoch === 1) {\n          job.progress.best_metric = valLoss;\n        } else if (valLoss < job.progress.best_metric) {\n          job.progress.best_metric = valLoss;\n        }\n        \n        // Estimate remaining time\n        const elapsed = Date.now() - (job.started_at?.getTime() || Date.now());\n        job.progress.elapsed_time = elapsed;\n        const remaining = (elapsed / job.progress.current_step) * (totalSteps - job.progress.current_step);\n        job.progress.estimated_remaining = remaining;\n        \n        // Emit progress update\n        if (step % 10 === 0) {\n          this.emit('trainingProgress', {\n            jobId: job.id,\n            progress: job.progress,\n            metrics: {\n              train_loss: trainLoss,\n              validation_loss: valLoss,\n              learning_rate: lr\n            }\n          });\n        }\n        \n        // Simulate training time\n        await new Promise(resolve => setTimeout(resolve, 100));\n      }\n      \n      this.addTrainingLog(job, 'info', `Completed epoch ${epoch}/${job.config.hyperparameters.epochs}`);\n      \n      // Save checkpoint\n      const checkpointPath = `/tmp/checkpoint_${job.id}_epoch_${epoch}.pth`;\n      job.artifacts.checkpoints.push(checkpointPath);\n      \n      // Early stopping check\n      if (job.config.optimization.early_stopping.enabled) {\n        const shouldStop = await this.checkEarlyStopping(job, epoch);\n        if (shouldStop) {\n          this.addTrainingLog(job, 'info', `Early stopping triggered at epoch ${epoch}`);\n          break;\n        }\n      }\n    }\n  }\n\n  private async checkEarlyStopping(job: TrainingJob, currentEpoch: number): Promise<boolean> {\n    const { early_stopping } = job.config.optimization;\n    if (!early_stopping.enabled || currentEpoch < early_stopping.patience) {\n      return false;\n    }\n    \n    const recentLosses = job.metrics.validation_loss.slice(-early_stopping.patience);\n    const bestRecentLoss = Math.min(...recentLosses);\n    const currentLoss = job.progress.validation_loss;\n    \n    return (currentLoss - bestRecentLoss) < early_stopping.min_delta;\n  }\n\n  private async evaluateTrainedModel(job: TrainingJob): Promise<any> {\n    // Simulate model evaluation\n    return {\n      accuracy: 0.85 + Math.random() * 0.1,\n      f1_score: 0.80 + Math.random() * 0.15,\n      perplexity: 15 + Math.random() * 10,\n      inference_time: 100 + Math.random() * 50,\n      memory_usage: 512 + Math.random() * 256,\n      throughput: 50 + Math.random() * 20\n    };\n  }\n\n  private async createModelVersion(job: TrainingJob, evaluation: any): Promise<TrainingModelVersion> {\n    const versionId = `${job.config.model}_v${Date.now()}`;\n    \n    const modelVersion: TrainingModelVersion = {\n      id: versionId,\n      model_name: job.config.model,\n      version: `1.0.${Date.now()}`,\n      base_model: job.config.metadata.base_model || job.config.model,\n      training_job_id: job.id,\n      performance_metrics: evaluation,\n      model_size: Math.round(1000 + Math.random() * 5000), // MB\n      deployment_status: 'pending',\n      created_at: new Date()\n    };\n    \n    // Add to registry\n    if (!this.modelRegistry.has(job.config.model)) {\n      this.modelRegistry.set(job.config.model, []);\n    }\n    this.modelRegistry.get(job.config.model)!.push(modelVersion);\n    \n    return modelVersion;\n  }\n\n  private handleTrainingError(job: TrainingJob, error: any): void {\n    job.status = 'failed';\n    job.error = {\n      message: error instanceof Error ? error.message : 'Unknown error',\n      stack: error instanceof Error ? error.stack : undefined,\n      timestamp: new Date()\n    };\n    \n    this.addTrainingLog(job, 'error', `Training failed: ${job.error.message}`);\n    \n    this.emit('jobFailed', {\n      jobId: job.id,\n      error: job.error\n    });\n  }\n\n  private addTrainingLog(job: TrainingJob, level: TrainingLog['level'], message: string, metadata?: any): void {\n    job.logs.push({\n      timestamp: new Date(),\n      level,\n      message,\n      metadata\n    });\n    \n    // Keep only last 1000 logs to prevent memory issues\n    if (job.logs.length > 1000) {\n      job.logs = job.logs.slice(-1000);\n    }\n  }\n\n  private async generatePromptVariation(template: string, config: any): Promise<string> {\n    // Generate variations of the template prompt\n    const variationPrompt = `Create a variation of this prompt template that maintains the same purpose but uses different wording:\n\nTemplate: \"${template}\"\n\nReturn only the varied prompt without explanations.`;\n\n    try {\n      const response = await llmService.generate(variationPrompt, undefined, {\n        temperature: config.temperature,\n        num_predict: Math.min(500, config.max_tokens)\n      });\n      return response.response.trim();\n    } catch (error) {\n      console.warn('Failed to generate prompt variation, using original:', error);\n      return template;\n    }\n  }\n\n  private async assessDataQuality(prompt: string, response: string): Promise<number> {\n    // Simple quality assessment - replace with more sophisticated logic\n    let score = 0.5;\n    \n    // Check response length\n    if (response.length > 50 && response.length < 2000) score += 0.2;\n    \n    // Check for coherence (simple heuristic)\n    if (response.includes('.') && response.split('.').length > 1) score += 0.1;\n    \n    // Check for relevance (keyword matching)\n    const promptWords = prompt.toLowerCase().split(' ');\n    const responseWords = response.toLowerCase().split(' ');\n    const overlap = promptWords.filter(word => responseWords.includes(word)).length;\n    score += Math.min(0.2, overlap / promptWords.length);\n    \n    return Math.min(1.0, score);\n  }\n\n  private async calculateDatasetQuality(data: any[]): Promise<any> {\n    if (data.length === 0) {\n      return { coherence_score: 0, relevance_score: 0, diversity_score: 0, safety_score: 0 };\n    }\n    \n    let totalCoherence = 0;\n    let totalRelevance = 0;\n    let totalSafety = 0;\n    \n    for (const item of data) {\n      totalCoherence += await this.assessDataQuality(item.prompt, item.response);\n      totalRelevance += await this.assessDataQuality(item.prompt, item.response);\n      totalSafety += 0.9; // Assume high safety for generated data\n    }\n    \n    // Calculate diversity (unique prompts / total prompts)\n    const uniquePrompts = new Set(data.map(item => item.prompt.toLowerCase())).size;\n    const diversityScore = uniquePrompts / data.length;\n    \n    return {\n      coherence_score: totalCoherence / data.length,\n      relevance_score: totalRelevance / data.length,\n      diversity_score: diversityScore,\n      safety_score: totalSafety / data.length\n    };\n  }\n\n  private async saveGeneratedData(data: any[], outputPath: string, format: string): Promise<void> {\n    // Save data in specified format (implement actual file saving logic)\n    console.log(`Saving ${data.length} samples to ${outputPath} in ${format} format`);\n  }\n\n  private async runBenchmarkEvaluation(modelName: string, dataset: string): Promise<any> {\n    // Implement benchmark evaluation logic\n    return {\n      dataset_name: dataset,\n      metrics: {\n        accuracy: 0.75 + Math.random() * 0.2,\n        f1_score: 0.70 + Math.random() * 0.25,\n        bleu_score: 0.65 + Math.random() * 0.3\n      },\n      sample_count: 1000,\n      evaluation_time: Date.now()\n    };\n  }\n\n  private async calculateCustomMetric(modelName: string, metric: string): Promise<number> {\n    // Implement custom metric calculation\n    return 0.8 + Math.random() * 0.2;\n  }\n\n  private calculateOverallScore(metrics: Record<string, number>): number {\n    const weights = {\n      accuracy: 0.3,\n      f1_score: 0.3,\n      bleu_score: 0.2,\n      inference_time: -0.1, // Negative weight for latency\n      memory_usage: -0.1    // Negative weight for memory usage\n    };\n    \n    let score = 0;\n    let totalWeight = 0;\n    \n    Object.entries(metrics).forEach(([metric, value]) => {\n      const weight = weights[metric] || 0.1;\n      score += value * weight;\n      totalWeight += Math.abs(weight);\n    });\n    \n    return totalWeight > 0 ? Math.max(0, Math.min(1, score / totalWeight)) : 0.5;\n  }\n\n  private async generatePerformanceRecommendations(\n    modelName: string,\n    metrics: Record<string, number>,\n    benchmarkResults: Record<string, any>\n  ): Promise<string[]> {\n    const recommendations: string[] = [];\n    \n    if (metrics.accuracy < 0.8) {\n      recommendations.push('Consider increasing training data or adjusting hyperparameters to improve accuracy');\n    }\n    \n    if (metrics.inference_time > 200) {\n      recommendations.push('Model inference time is high. Consider model optimization or quantization');\n    }\n    \n    if (metrics.memory_usage > 1000) {\n      recommendations.push('High memory usage detected. Consider model pruning or compression');\n    }\n    \n    return recommendations;\n  }\n\n  private async getModelVersion(versionId: string): Promise<TrainingModelVersion | null> {\n    for (const versions of this.modelRegistry.values()) {\n      const version = versions.find(v => v.id === versionId);\n      if (version) return version;\n    }\n    return null;\n  }\n\n  private async updateModelVersion(version: TrainingModelVersion): Promise<void> {\n    // Update model version in registry and storage\n    console.log(`Updated model version: ${version.id}`);\n  }\n\n  private async deployToOllama(version: TrainingModelVersion, config: any): Promise<any> {\n    // Implement Ollama deployment\n    return { status: 'success', endpoint: `http://localhost:11434/api/generate` };\n  }\n\n  private async deployToHuggingFace(version: TrainingModelVersion, config: any): Promise<any> {\n    // Implement HuggingFace deployment\n    return { status: 'success', endpoint: `https://huggingface.co/models/${version.model_name}` };\n  }\n\n  private async deployToLocal(version: TrainingModelVersion, config: any): Promise<any> {\n    // Implement local deployment\n    return { status: 'success', endpoint: `http://localhost:8080/api/generate` };\n  }\n\n  private async deployToCloud(version: TrainingModelVersion, config: any): Promise<any> {\n    // Implement cloud deployment\n    return { status: 'success', endpoint: `https://api.cloud-provider.com/models/${version.id}` };\n  }\n}\n\n// Export singleton instance\nexport const modelTrainingEngine = new ModelTrainingEngine();"],"names":["ModelTrainingEngine","modelTrainingEngine","EventEmitter","initialize","isInitialized","console","log","initializeTensorFlow","loadModelRegistry","resumeInterruptedJobs","emit","timestamp","Date","error","createTrainingJob","config","jobId","now","Math","random","toString","substr","fullConfig","id","metadata","created_by","created_at","tags","description","base_model","model","training_objective","validateTrainingConfig","job","status","progress","current_epoch","total_epochs","hyperparameters","epochs","current_step","total_steps","elapsed_time","estimated_remaining","train_loss","validation_loss","best_metric","metrics","training_loss","learning_rate","custom_metrics","logs","artifacts","checkpoints","evaluation_reports","activeJobs","set","eventStore","recordEvent","event_type","entity_id","entity_type","data","startTrainingJob","get","Error","started_at","addTrainingLog","executeTrainingJob","catch","handleTrainingError","generateSyntheticData","generationId","generated_samples","total_samples","generation_config","num_samples","current_template","total_templates","template_prompts","length","generatedData","templateIndex","template","samplesPerTemplate","ceil","i","prompt","generatePromptVariation","response","llmService","generate","undefined","temperature","num_predict","max_tokens","quality_filter","qualityScore","assessDataQuality","push","template_id","generation_id","warn","quality_metrics","calculateDatasetQuality","outputPath","output_format","saveGeneratedData","output_path","samples_generated","evaluateModel","modelName","benchmarkDatasets","customMetrics","evaluationId","benchmarkResults","detailedMetrics","dataset","result","runBenchmarkEvaluation","Object","entries","forEach","metric","value","keys","calculateCustomMetric","overallScore","calculateOverallScore","recommendations","generatePerformanceRecommendations","evaluationResult","overall_score","detailed_metrics","benchmark_results","model_name","deployModel","modelVersionId","target","deploymentId","modelVersion","getModelVersion","deploymentResult","deployToOllama","deployToHuggingFace","deployToLocal","deployToCloud","deployment_status","deployed_at","updateModelVersion","model_version_id","deployment_id","message","getTrainingJob","listTrainingJobs","filters","jobs","Array","from","values","filter","created_after","created_before","cancelTrainingJob","includes","getModelRegistry","Map","modelRegistry","getTrainingStatistics","completedJobs","failedJobs","averageTrainingTime","reduce","sum","totalTrainingHours","modelsDeployed","flat","version","total_jobs","successful_jobs","failed_jobs","average_training_time","models_deployed","total_training_hours","tf","setBackend","trainingData","validation_split","simulateTraining","evaluation","evaluateTrainedModel","createModelVersion","completed_at","performance_metrics","totalSteps","epoch","step","trainLoss","max","exp","valLoss","lr","pow","elapsed","getTime","remaining","Promise","resolve","setTimeout","checkpointPath","optimization","early_stopping","enabled","shouldStop","checkEarlyStopping","currentEpoch","patience","recentLosses","slice","bestRecentLoss","min","currentLoss","min_delta","accuracy","f1_score","perplexity","inference_time","memory_usage","throughput","versionId","training_job_id","model_size","round","has","stack","level","variationPrompt","trim","score","split","promptWords","toLowerCase","responseWords","overlap","word","coherence_score","relevance_score","diversity_score","safety_score","totalCoherence","totalRelevance","totalSafety","item","uniquePrompts","Set","map","size","diversityScore","format","dataset_name","bleu_score","sample_count","evaluation_time","weights","totalWeight","weight","abs","versions","find","v","endpoint","modelHealthMonitor","optimizationEngine","trainingCache","performanceMetrics","EventStore","getInstance","ModelHealthMonitor","healthCheckInterval","benchmarkInterval","maxResponseTime","maxErrorRate","minHealthScore","alertThresholds","responseTime","errorRate","memoryUsage","OptimizationEngine","LRUCache","ttl"],"mappings":";;;;;;;;;;;QAiKaA;eAAAA;;QAk6BAC;eAAAA;;;wBAnkCgB;4BACF;oCACQ;oCACA;4BACR;0BACF;kEAEL;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0Jb,MAAMD,4BAA4BE,oBAAY;IAwCnD,MAAMC,aAA4B;QAChC,IAAI,IAAI,CAACC,aAAa,EAAE;QAExB,IAAI;YACFC,QAAQC,GAAG,CAAC;YAEZ,gCAAgC;YAChC,MAAM,IAAI,CAACC,oBAAoB;YAE/B,+BAA+B;YAC/B,MAAM,IAAI,CAACC,iBAAiB;YAE5B,uCAAuC;YACvC,MAAM,IAAI,CAACC,qBAAqB;YAEhC,IAAI,CAACL,aAAa,GAAG;YACrBC,QAAQC,GAAG,CAAC;YAEZ,IAAI,CAACI,IAAI,CAAC,eAAe;gBAAEC,WAAW,IAAIC;YAAO;QACnD,EAAE,OAAOC,OAAO;YACdR,QAAQQ,KAAK,CAAC,iDAAiDA;YAC/D,MAAMA;QACR;IACF;IAEA;;GAEC,GACD,MAAMC,kBAAkBC,MAAsD,EAAwB;QACpG,MAAMC,QAAQ,CAAC,MAAM,EAAEJ,KAAKK,GAAG,GAAG,CAAC,EAAEC,KAAKC,MAAM,GAAGC,QAAQ,CAAC,IAAIC,MAAM,CAAC,GAAG,IAAI;QAE9E,MAAMC,aAAoC;YACxC,GAAGP,MAAM;YACTQ,IAAIP;YACJQ,UAAU;gBACRC,YAAY;gBACZC,YAAY,IAAId;gBAChBe,MAAM,EAAE;gBACRC,aAAa;gBACbC,YAAYd,OAAOe,KAAK;gBACxBC,oBAAoB;YACtB;QACF;QAEA,yBAAyB;QACzB,MAAM,IAAI,CAACC,sBAAsB,CAACV;QAElC,MAAMW,MAAmB;YACvBV,IAAIP;YACJD,QAAQO;YACRY,QAAQ;YACRC,UAAU;gBACRC,eAAe;gBACfC,cAActB,OAAOuB,eAAe,CAACC,MAAM;gBAC3CC,cAAc;gBACdC,aAAa;gBACbC,cAAc;gBACdC,qBAAqB;gBACrBC,YAAY;gBACZC,iBAAiB;gBACjBC,aAAa;YACf;YACAC,SAAS;gBACPC,eAAe,EAAE;gBACjBH,iBAAiB,EAAE;gBACnBI,eAAe,EAAE;gBACjBC,gBAAgB,CAAC;YACnB;YACAC,MAAM,EAAE;YACRC,WAAW;gBACTC,aAAa,EAAE;gBACfC,oBAAoB,EAAE;YACxB;QACF;QAEA,IAAI,CAACC,UAAU,CAACC,GAAG,CAACxC,OAAOiB;QAE3B,sBAAsB;QACtB,MAAM,IAAI,CAACwB,UAAU,CAACC,WAAW,CAAC;YAChCC,YAAY;YACZC,WAAW5C;YACX6C,aAAa;YACbC,MAAM;gBAAE/C,QAAQO;YAAW;YAC3BX,WAAW,IAAIC;QACjB;QAEA,IAAI,CAACF,IAAI,CAAC,cAAc;YAAEM;YAAOD,QAAQO;QAAW;QAEpD,OAAOW;IACT;IAEA;;GAEC,GACD,MAAM8B,iBAAiB/C,KAAa,EAAiB;QACnD,MAAMiB,MAAM,IAAI,CAACsB,UAAU,CAACS,GAAG,CAAChD;QAChC,IAAI,CAACiB,KAAK;YACR,MAAM,IAAIgC,MAAM,CAAC,aAAa,EAAEjD,MAAM,UAAU,CAAC;QACnD;QAEA,IAAIiB,IAAIC,MAAM,KAAK,WAAW;YAC5B,MAAM,IAAI+B,MAAM,CAAC,aAAa,EAAEjD,MAAM,yBAAyB,CAAC;QAClE;QAEAiB,IAAIC,MAAM,GAAG;QACbD,IAAIiC,UAAU,GAAG,IAAItD;QAErB,IAAI,CAACuD,cAAc,CAAClC,KAAK,QAAQ;QAEjC,+BAA+B;QAC/B,IAAI,CAACmC,kBAAkB,CAACnC,KAAKoC,KAAK,CAACxD,CAAAA;YACjC,IAAI,CAACyD,mBAAmB,CAACrC,KAAKpB;QAChC;QAEA,MAAM,IAAI,CAAC4C,UAAU,CAACC,WAAW,CAAC;YAChCC,YAAY;YACZC,WAAW5C;YACX6C,aAAa;YACbC,MAAM;gBAAE5B,QAAQD,IAAIC,MAAM;YAAC;YAC3BvB,WAAW,IAAIC;QACjB;QAEA,IAAI,CAACF,IAAI,CAAC,cAAc;YAAEM;YAAOkB,QAAQD,IAAIC,MAAM;QAAC;IACtD;IAEA;;GAEC,GACD,MAAMqC,sBAAsBxD,MAA+B,EAAmB;QAC5E,MAAMyD,eAAe,CAAC,MAAM,EAAE5D,KAAKK,GAAG,GAAG,CAAC,EAAEC,KAAKC,MAAM,GAAGC,QAAQ,CAAC,IAAIC,MAAM,CAAC,GAAG,IAAI;QAErF,IAAI;YACFN,OAAOmB,MAAM,GAAG;YAChBnB,OAAOoB,QAAQ,GAAG;gBAChBsC,mBAAmB;gBACnBC,eAAe3D,OAAO4D,iBAAiB,CAACC,WAAW;gBACnDC,kBAAkB;gBAClBC,iBAAiB/D,OAAOgE,gBAAgB,CAACC,MAAM;YACjD;YAEA,MAAMC,gBAAuB,EAAE;YAE/B,IAAK,IAAIC,gBAAgB,GAAGA,gBAAgBnE,OAAOgE,gBAAgB,CAACC,MAAM,EAAEE,gBAAiB;gBAC3F,MAAMC,WAAWpE,OAAOgE,gBAAgB,CAACG,cAAc;gBACvDnE,OAAOoB,QAAQ,CAAC0C,gBAAgB,GAAGK;gBAEnC,MAAME,qBAAqBlE,KAAKmE,IAAI,CAACtE,OAAO4D,iBAAiB,CAACC,WAAW,GAAG7D,OAAOgE,gBAAgB,CAACC,MAAM;gBAE1G,IAAK,IAAIM,IAAI,GAAGA,IAAIF,sBAAsBrE,OAAOoB,QAAQ,CAACsC,iBAAiB,GAAG1D,OAAO4D,iBAAiB,CAACC,WAAW,EAAEU,IAAK;oBACvH,IAAI;wBACF,qCAAqC;wBACrC,MAAMC,SAAS,MAAM,IAAI,CAACC,uBAAuB,CAACL,UAAUpE,OAAO4D,iBAAiB;wBAEpF,8BAA8B;wBAC9B,MAAMc,WAAW,MAAMC,sBAAU,CAACC,QAAQ,CAACJ,QAAQK,WAAW;4BAC5DC,aAAa9E,OAAO4D,iBAAiB,CAACkB,WAAW;4BACjDC,aAAa/E,OAAO4D,iBAAiB,CAACoB,UAAU;wBAClD;wBAEA,qCAAqC;wBACrC,IAAIhF,OAAO4D,iBAAiB,CAACqB,cAAc,EAAE;4BAC3C,MAAMC,eAAe,MAAM,IAAI,CAACC,iBAAiB,CAACX,QAAQE,SAASA,QAAQ;4BAC3E,IAAIQ,eAAe,KAAK,UAAU,2BAA2B;wBAC/D;wBAEAhB,cAAckB,IAAI,CAAC;4BACjBZ,QAAQA;4BACRE,UAAUA,SAASA,QAAQ;4BAC3BW,aAAalB;4BACbmB,eAAe7B;4BACf7D,WAAW,IAAIC;wBACjB;wBAEAG,OAAOoB,QAAQ,CAACsC,iBAAiB;oBACnC,EAAE,OAAO5D,OAAO;wBACdR,QAAQiG,IAAI,CAAC,CAAC,0BAA0B,EAAEhB,EAAE,cAAc,EAAEJ,cAAc,CAAC,CAAC,EAAErE;oBAChF;gBACF;YACF;YAEA,4BAA4B;YAC5BE,OAAOwF,eAAe,GAAG,MAAM,IAAI,CAACC,uBAAuB,CAACvB;YAE5D,sBAAsB;YACtB,MAAMwB,aAAa,CAAC,oBAAoB,EAAEjC,aAAa,CAAC,EAAEzD,OAAO2F,aAAa,EAAE;YAChF,MAAM,IAAI,CAACC,iBAAiB,CAAC1B,eAAewB,YAAY1F,OAAO2F,aAAa;YAE5E3F,OAAOmB,MAAM,GAAG;YAEhB,MAAM,IAAI,CAACuB,UAAU,CAACC,WAAW,CAAC;gBAChCC,YAAY;gBACZC,WAAWY;gBACXX,aAAa;gBACbC,MAAM;oBACJ/C;oBACA6F,aAAaH;oBACbI,mBAAmB5B,cAAcD,MAAM;gBACzC;gBACArE,WAAW,IAAIC;YACjB;YAEA,OAAO6F;QACT,EAAE,OAAO5F,OAAO;YACdE,OAAOmB,MAAM,GAAG;YAChB7B,QAAQQ,KAAK,CAAC,sCAAsCA;YACpD,MAAMA;QACR;IACF;IAEA;;GAEC,GACD,MAAMiG,cACJC,SAAiB,EACjBC,iBAA2B,EAC3BC,gBAA0B,EAAE,EAM3B;QACD,MAAMC,eAAe,CAAC,KAAK,EAAEtG,KAAKK,GAAG,GAAG,CAAC,EAAEC,KAAKC,MAAM,GAAGC,QAAQ,CAAC,IAAIC,MAAM,CAAC,GAAG,IAAI;QAEpF,IAAI;YACFhB,QAAQC,GAAG,CAAC,CAAC,qBAAqB,EAAEyG,WAAW;YAE/C,MAAMI,mBAAwC,CAAC;YAC/C,MAAMC,kBAA0C,CAAC;YAEjD,4BAA4B;YAC5B,KAAK,MAAMC,WAAWL,kBAAmB;gBACvC,MAAMM,SAAS,MAAM,IAAI,CAACC,sBAAsB,CAACR,WAAWM;gBAC5DF,gBAAgB,CAACE,QAAQ,GAAGC;gBAE5B,oBAAoB;gBACpBE,OAAOC,OAAO,CAACH,OAAOvE,OAAO,EAAE2E,OAAO,CAAC,CAAC,CAACC,QAAQC,MAAM;oBACrD,IAAI,CAACR,eAAe,CAACO,OAAO,EAAEP,eAAe,CAACO,OAAO,GAAG;oBACxDP,eAAe,CAACO,OAAO,IAAIC;gBAC7B;YACF;YAEA,kCAAkC;YAClCJ,OAAOK,IAAI,CAACT,iBAAiBM,OAAO,CAACC,CAAAA;gBACnCP,eAAe,CAACO,OAAO,IAAIX,kBAAkBhC,MAAM;YACrD;YAEA,qBAAqB;YACrB,KAAK,MAAM2C,UAAUV,cAAe;gBAClC,MAAMW,QAAQ,MAAM,IAAI,CAACE,qBAAqB,CAACf,WAAWY;gBAC1DP,eAAe,CAACO,OAAO,GAAGC;YAC5B;YAEA,0BAA0B;YAC1B,MAAMG,eAAe,IAAI,CAACC,qBAAqB,CAACZ;YAEhD,2BAA2B;YAC3B,MAAMa,kBAAkB,MAAM,IAAI,CAACC,kCAAkC,CACnEnB,WACAK,iBACAD;YAGF,MAAMgB,mBAAmB;gBACvBC,eAAeL;gBACfM,kBAAkBjB;gBAClBkB,mBAAmBnB;gBACnBc;YACF;YAEA,2BAA2B;YAC3B,MAAM,IAAI,CAACxE,UAAU,CAACC,WAAW,CAAC;gBAChCC,YAAY;gBACZC,WAAWsD;gBACXrD,aAAa;gBACbC,MAAM;oBACJyE,YAAYxB;oBACZ,GAAGoB,gBAAgB;gBACrB;gBACAxH,WAAW,IAAIC;YACjB;YAEA,OAAOuH;QACT,EAAE,OAAOtH,OAAO;YACdR,QAAQQ,KAAK,CAAC,CAAC,yBAAyB,EAAEkG,UAAU,CAAC,CAAC,EAAElG;YACxD,MAAMA;QACR;IACF;IAEA;;GAEC,GACD,MAAM2H,YACJC,cAAsB,EACtBC,MAAoD,EACpD3H,SAII,CAAC,CAAC,EAML;QACD,MAAM4H,eAAe,CAAC,OAAO,EAAE/H,KAAKK,GAAG,GAAG,CAAC,EAAEC,KAAKC,MAAM,GAAGC,QAAQ,CAAC,IAAIC,MAAM,CAAC,GAAG,IAAI;QAEtF,IAAI;YACFhB,QAAQC,GAAG,CAAC,CAAC,4BAA4B,EAAEmI,eAAe,IAAI,EAAEC,QAAQ;YAExE,4BAA4B;YAC5B,MAAME,eAAe,MAAM,IAAI,CAACC,eAAe,CAACJ;YAChD,IAAI,CAACG,cAAc;gBACjB,MAAM,IAAI3E,MAAM,CAAC,cAAc,EAAEwE,eAAe,UAAU,CAAC;YAC7D;YAEA,IAAIK;YAEJ,OAAQJ;gBACN,KAAK;oBACHI,mBAAmB,MAAM,IAAI,CAACC,cAAc,CAACH,cAAc7H;oBAC3D;gBACF,KAAK;oBACH+H,mBAAmB,MAAM,IAAI,CAACE,mBAAmB,CAACJ,cAAc7H;oBAChE;gBACF,KAAK;oBACH+H,mBAAmB,MAAM,IAAI,CAACG,aAAa,CAACL,cAAc7H;oBAC1D;gBACF,KAAK;oBACH+H,mBAAmB,MAAM,IAAI,CAACI,aAAa,CAACN,cAAc7H;oBAC1D;gBACF;oBACE,MAAM,IAAIkD,MAAM,CAAC,+BAA+B,EAAEyE,QAAQ;YAC9D;YAEA,8BAA8B;YAC9BE,aAAaO,iBAAiB,GAAGL,iBAAiB5G,MAAM,KAAK,YAAY,aAAa;YACtF,IAAI4G,iBAAiB5G,MAAM,KAAK,WAAW;gBACzC0G,aAAaQ,WAAW,GAAG,IAAIxI;YACjC;YAEA,MAAM,IAAI,CAACyI,kBAAkB,CAACT;YAE9B,oBAAoB;YACpB,MAAM,IAAI,CAACnF,UAAU,CAACC,WAAW,CAAC;gBAChCC,YAAY;gBACZC,WAAW+E;gBACX9E,aAAa;gBACbC,MAAM;oBACJwF,kBAAkBb;oBAClBC;oBACA3H;oBACAuG,QAAQwB;gBACV;gBACAnI,WAAW,IAAIC;YACjB;YAEA,OAAO;gBACL2I,eAAeZ;gBACf,GAAGG,gBAAgB;YACrB;QACF,EAAE,OAAOjI,OAAO;YACdR,QAAQQ,KAAK,CAAC,CAAC,uBAAuB,EAAE4H,eAAe,CAAC,CAAC,EAAE5H;YAE3D,MAAM,IAAI,CAAC4C,UAAU,CAACC,WAAW,CAAC;gBAChCC,YAAY;gBACZC,WAAW+E;gBACX9E,aAAa;gBACbC,MAAM;oBACJwF,kBAAkBb;oBAClBC;oBACA7H,OAAOA,iBAAiBoD,QAAQpD,MAAM2I,OAAO,GAAG;gBAClD;gBACA7I,WAAW,IAAIC;YACjB;YAEA,MAAMC;QACR;IACF;IAEA;;GAEC,GACD4I,eAAezI,KAAa,EAA2B;QACrD,OAAO,IAAI,CAACuC,UAAU,CAACS,GAAG,CAAChD;IAC7B;IAEA;;GAEC,GACD0I,iBAAiBC,UAKb,CAAC,CAAC,EAAiB;QACrB,MAAMC,OAAOC,MAAMC,IAAI,CAAC,IAAI,CAACvG,UAAU,CAACwG,MAAM;QAE9C,OAAOH,KAAKI,MAAM,CAAC/H,CAAAA;YACjB,IAAI0H,QAAQzH,MAAM,IAAID,IAAIC,MAAM,KAAKyH,QAAQzH,MAAM,EAAE,OAAO;YAC5D,IAAIyH,QAAQ7H,KAAK,IAAIG,IAAIlB,MAAM,CAACe,KAAK,KAAK6H,QAAQ7H,KAAK,EAAE,OAAO;YAChE,IAAI6H,QAAQM,aAAa,IAAIhI,IAAIlB,MAAM,CAACS,QAAQ,CAACE,UAAU,GAAGiI,QAAQM,aAAa,EAAE,OAAO;YAC5F,IAAIN,QAAQO,cAAc,IAAIjI,IAAIlB,MAAM,CAACS,QAAQ,CAACE,UAAU,GAAGiI,QAAQO,cAAc,EAAE,OAAO;YAC9F,OAAO;QACT;IACF;IAEA;;GAEC,GACD,MAAMC,kBAAkBnJ,KAAa,EAAiB;QACpD,MAAMiB,MAAM,IAAI,CAACsB,UAAU,CAACS,GAAG,CAAChD;QAChC,IAAI,CAACiB,KAAK;YACR,MAAM,IAAIgC,MAAM,CAAC,aAAa,EAAEjD,MAAM,UAAU,CAAC;QACnD;QAEA,IAAI,CAAC;YAAC;YAAW;YAAgB;SAAW,CAACoJ,QAAQ,CAACnI,IAAIC,MAAM,GAAG;YACjE,MAAM,IAAI+B,MAAM,CAAC,aAAa,EAAEjD,MAAM,gCAAgC,EAAEiB,IAAIC,MAAM,EAAE;QACtF;QAEAD,IAAIC,MAAM,GAAG;QACb,IAAI,CAACiC,cAAc,CAAClC,KAAK,QAAQ;QAEjC,MAAM,IAAI,CAACwB,UAAU,CAACC,WAAW,CAAC;YAChCC,YAAY;YACZC,WAAW5C;YACX6C,aAAa;YACbC,MAAM;gBAAE5B,QAAQD,IAAIC,MAAM;YAAC;YAC3BvB,WAAW,IAAIC;QACjB;QAEA,IAAI,CAACF,IAAI,CAAC,gBAAgB;YAAEM;YAAOkB,QAAQD,IAAIC,MAAM;QAAC;IACxD;IAEA;;GAEC,GACDmI,mBAAwD;QACtD,OAAO,IAAIC,IAAI,IAAI,CAACC,aAAa;IACnC;IAEA;;GAEC,GACDC,wBAOE;QACA,MAAMZ,OAAOC,MAAMC,IAAI,CAAC,IAAI,CAACvG,UAAU,CAACwG,MAAM;QAC9C,MAAMU,gBAAgBb,KAAKI,MAAM,CAAC/H,CAAAA,MAAOA,IAAIC,MAAM,KAAK;QACxD,MAAMwI,aAAad,KAAKI,MAAM,CAAC/H,CAAAA,MAAOA,IAAIC,MAAM,KAAK;QAErD,MAAMyI,sBAAsBF,cAAczF,MAAM,GAAG,IAC/CyF,cAAcG,MAAM,CAAC,CAACC,KAAK5I,MAAQ4I,MAAM5I,IAAIE,QAAQ,CAACO,YAAY,EAAE,KAAK+H,cAAczF,MAAM,GAC7F;QAEJ,MAAM8F,qBAAqBlB,KAAKgB,MAAM,CAAC,CAACC,KAAK5I,MAAQ4I,MAAM5I,IAAIE,QAAQ,CAACO,YAAY,EAAE,KAAM,CAAA,OAAO,KAAK,EAAC;QAEzG,MAAMqI,iBAAiBlB,MAAMC,IAAI,CAAC,IAAI,CAACS,aAAa,CAACR,MAAM,IACxDiB,IAAI,GACJhB,MAAM,CAACiB,CAAAA,UAAWA,QAAQ9B,iBAAiB,KAAK,YAAYnE,MAAM;QAErE,OAAO;YACLkG,YAAYtB,KAAK5E,MAAM;YACvBmG,iBAAiBV,cAAczF,MAAM;YACrCoG,aAAaV,WAAW1F,MAAM;YAC9BqG,uBAAuBV;YACvBW,iBAAiBP;YACjBQ,sBAAsBT;QACxB;IACF;IAEA,kBAAkB;IAClB,MAAcvK,uBAAsC;QAClD,IAAI;YACF,yBAAyB;YACzBiL,UAAGC,UAAU,CAAC;YACdpL,QAAQC,GAAG,CAAC;QACd,EAAE,OAAOO,OAAO;YACdR,QAAQiG,IAAI,CAAC;QACf;IACF;IAEA,MAAc9F,oBAAmC;QAC/C,IAAI;YACF,qDAAqD;YACrD,mDAAmD;YACnDH,QAAQC,GAAG,CAAC;QACd,EAAE,OAAOO,OAAO;YACdR,QAAQiG,IAAI,CAAC,qCAAqCzF;QACpD;IACF;IAEA,MAAcJ,wBAAuC;QACnD,IAAI;YACF,iDAAiD;YACjDJ,QAAQC,GAAG,CAAC;QACd,EAAE,OAAOO,OAAO;YACdR,QAAQiG,IAAI,CAAC,yCAAyCzF;QACxD;IACF;IAEA,MAAcmB,uBAAuBjB,MAA6B,EAAiB;QACjF,kCAAkC;QAClC,IAAI,CAACA,OAAOe,KAAK,EAAE;YACjB,MAAM,IAAImC,MAAM;QAClB;QAEA,IAAIlD,OAAOuB,eAAe,CAACC,MAAM,IAAI,GAAG;YACtC,MAAM,IAAI0B,MAAM;QAClB;QAEA,IAAIlD,OAAOuB,eAAe,CAACW,aAAa,IAAI,KAAKlC,OAAOuB,eAAe,CAACW,aAAa,GAAG,GAAG;YACzF,MAAM,IAAIgB,MAAM;QAClB;QAEA,IAAIlD,OAAO2K,YAAY,CAACC,gBAAgB,IAAI,KAAK5K,OAAO2K,YAAY,CAACC,gBAAgB,IAAI,GAAG;YAC1F,MAAM,IAAI1H,MAAM;QAClB;IACF;IAEA,MAAcG,mBAAmBnC,GAAgB,EAAiB;QAChE,IAAI;YACFA,IAAIC,MAAM,GAAG;YACb,IAAI,CAACiC,cAAc,CAAClC,KAAK,QAAQ;YAEjC,iEAAiE;YACjE,MAAM,IAAI,CAAC2J,gBAAgB,CAAC3J;YAE5BA,IAAIC,MAAM,GAAG;YACb,IAAI,CAACiC,cAAc,CAAClC,KAAK,QAAQ;YAEjC,yBAAyB;YACzB,MAAM4J,aAAa,MAAM,IAAI,CAACC,oBAAoB,CAAC7J;YAEnD,uBAAuB;YACvB,MAAM2G,eAAe,MAAM,IAAI,CAACmD,kBAAkB,CAAC9J,KAAK4J;YAExD5J,IAAIC,MAAM,GAAG;YACbD,IAAI+J,YAAY,GAAG,IAAIpL;YAEvB,IAAI,CAACuD,cAAc,CAAClC,KAAK,QAAQ,CAAC,gDAAgD,EAAE2G,aAAarH,EAAE,EAAE;YAErG,MAAM,IAAI,CAACkC,UAAU,CAACC,WAAW,CAAC;gBAChCC,YAAY;gBACZC,WAAW3B,IAAIV,EAAE;gBACjBsC,aAAa;gBACbC,MAAM;oBACJwF,kBAAkBV,aAAarH,EAAE;oBACjC0K,qBAAqBrD,aAAaqD,mBAAmB;gBACvD;gBACAtL,WAAW,IAAIC;YACjB;YAEA,IAAI,CAACF,IAAI,CAAC,gBAAgB;gBACxBM,OAAOiB,IAAIV,EAAE;gBACbkH,gBAAgBG,aAAarH,EAAE;gBAC/BwB,SAAS6F,aAAaqD,mBAAmB;YAC3C;QAEF,EAAE,OAAOpL,OAAO;YACd,IAAI,CAACyD,mBAAmB,CAACrC,KAAKpB;QAChC;IACF;IAEA,MAAc+K,iBAAiB3J,GAAgB,EAAiB;QAC9D,4DAA4D;QAC5D,MAAMiK,aAAajK,IAAIlB,MAAM,CAACuB,eAAe,CAACC,MAAM,GAAG,KAAK,6BAA6B;QACzFN,IAAIE,QAAQ,CAACM,WAAW,GAAGyJ;QAE3B,IAAK,IAAIC,QAAQ,GAAGA,SAASlK,IAAIlB,MAAM,CAACuB,eAAe,CAACC,MAAM,EAAE4J,QAAS;YACvElK,IAAIE,QAAQ,CAACC,aAAa,GAAG+J;YAE7B,IAAK,IAAIC,OAAO,GAAGA,QAAQ,KAAKA,OAAQ;gBACtC,IAAInK,IAAIC,MAAM,KAAK,aAAa;oBAC9B,MAAM,IAAI+B,MAAM;gBAClB;gBAEAhC,IAAIE,QAAQ,CAACK,YAAY,GAAG,AAAC2J,CAAAA,QAAQ,CAAA,IAAK,MAAMC;gBAEhD,4BAA4B;gBAC5B,MAAMC,YAAYnL,KAAKoL,GAAG,CAAC,KAAK,MAAMpL,KAAKqL,GAAG,CAAC,CAACtK,IAAIE,QAAQ,CAACK,YAAY,GAAG,QAAQtB,KAAKC,MAAM,KAAK;gBACpG,MAAMqL,UAAUH,YAAa,CAAA,MAAMnL,KAAKC,MAAM,KAAK,GAAE;gBACrD,MAAMsL,KAAKxK,IAAIlB,MAAM,CAACuB,eAAe,CAACW,aAAa,GAAG/B,KAAKwL,GAAG,CAAC,MAAMP,QAAQ;gBAE7ElK,IAAIE,QAAQ,CAACS,UAAU,GAAGyJ;gBAC1BpK,IAAIE,QAAQ,CAACU,eAAe,GAAG2J;gBAC/BvK,IAAIc,OAAO,CAACC,aAAa,CAACmD,IAAI,CAACkG;gBAC/BpK,IAAIc,OAAO,CAACF,eAAe,CAACsD,IAAI,CAACqG;gBACjCvK,IAAIc,OAAO,CAACE,aAAa,CAACkD,IAAI,CAACsG;gBAE/B,qBAAqB;gBACrB,IAAIL,SAAS,KAAKD,UAAU,GAAG;oBAC7BlK,IAAIE,QAAQ,CAACW,WAAW,GAAG0J;gBAC7B,OAAO,IAAIA,UAAUvK,IAAIE,QAAQ,CAACW,WAAW,EAAE;oBAC7Cb,IAAIE,QAAQ,CAACW,WAAW,GAAG0J;gBAC7B;gBAEA,0BAA0B;gBAC1B,MAAMG,UAAU/L,KAAKK,GAAG,KAAMgB,CAAAA,IAAIiC,UAAU,EAAE0I,aAAahM,KAAKK,GAAG,EAAC;gBACpEgB,IAAIE,QAAQ,CAACO,YAAY,GAAGiK;gBAC5B,MAAME,YAAY,AAACF,UAAU1K,IAAIE,QAAQ,CAACK,YAAY,GAAK0J,CAAAA,aAAajK,IAAIE,QAAQ,CAACK,YAAY,AAAD;gBAChGP,IAAIE,QAAQ,CAACQ,mBAAmB,GAAGkK;gBAEnC,uBAAuB;gBACvB,IAAIT,OAAO,OAAO,GAAG;oBACnB,IAAI,CAAC1L,IAAI,CAAC,oBAAoB;wBAC5BM,OAAOiB,IAAIV,EAAE;wBACbY,UAAUF,IAAIE,QAAQ;wBACtBY,SAAS;4BACPH,YAAYyJ;4BACZxJ,iBAAiB2J;4BACjBvJ,eAAewJ;wBACjB;oBACF;gBACF;gBAEA,yBAAyB;gBACzB,MAAM,IAAIK,QAAQC,CAAAA,UAAWC,WAAWD,SAAS;YACnD;YAEA,IAAI,CAAC5I,cAAc,CAAClC,KAAK,QAAQ,CAAC,gBAAgB,EAAEkK,MAAM,CAAC,EAAElK,IAAIlB,MAAM,CAACuB,eAAe,CAACC,MAAM,EAAE;YAEhG,kBAAkB;YAClB,MAAM0K,iBAAiB,CAAC,gBAAgB,EAAEhL,IAAIV,EAAE,CAAC,OAAO,EAAE4K,MAAM,IAAI,CAAC;YACrElK,IAAImB,SAAS,CAACC,WAAW,CAAC8C,IAAI,CAAC8G;YAE/B,uBAAuB;YACvB,IAAIhL,IAAIlB,MAAM,CAACmM,YAAY,CAACC,cAAc,CAACC,OAAO,EAAE;gBAClD,MAAMC,aAAa,MAAM,IAAI,CAACC,kBAAkB,CAACrL,KAAKkK;gBACtD,IAAIkB,YAAY;oBACd,IAAI,CAAClJ,cAAc,CAAClC,KAAK,QAAQ,CAAC,kCAAkC,EAAEkK,OAAO;oBAC7E;gBACF;YACF;QACF;IACF;IAEA,MAAcmB,mBAAmBrL,GAAgB,EAAEsL,YAAoB,EAAoB;QACzF,MAAM,EAAEJ,cAAc,EAAE,GAAGlL,IAAIlB,MAAM,CAACmM,YAAY;QAClD,IAAI,CAACC,eAAeC,OAAO,IAAIG,eAAeJ,eAAeK,QAAQ,EAAE;YACrE,OAAO;QACT;QAEA,MAAMC,eAAexL,IAAIc,OAAO,CAACF,eAAe,CAAC6K,KAAK,CAAC,CAACP,eAAeK,QAAQ;QAC/E,MAAMG,iBAAiBzM,KAAK0M,GAAG,IAAIH;QACnC,MAAMI,cAAc5L,IAAIE,QAAQ,CAACU,eAAe;QAEhD,OAAO,AAACgL,cAAcF,iBAAkBR,eAAeW,SAAS;IAClE;IAEA,MAAchC,qBAAqB7J,GAAgB,EAAgB;QACjE,4BAA4B;QAC5B,OAAO;YACL8L,UAAU,OAAO7M,KAAKC,MAAM,KAAK;YACjC6M,UAAU,OAAO9M,KAAKC,MAAM,KAAK;YACjC8M,YAAY,KAAK/M,KAAKC,MAAM,KAAK;YACjC+M,gBAAgB,MAAMhN,KAAKC,MAAM,KAAK;YACtCgN,cAAc,MAAMjN,KAAKC,MAAM,KAAK;YACpCiN,YAAY,KAAKlN,KAAKC,MAAM,KAAK;QACnC;IACF;IAEA,MAAc4K,mBAAmB9J,GAAgB,EAAE4J,UAAe,EAAiC;QACjG,MAAMwC,YAAY,GAAGpM,IAAIlB,MAAM,CAACe,KAAK,CAAC,EAAE,EAAElB,KAAKK,GAAG,IAAI;QAEtD,MAAM2H,eAAqC;YACzCrH,IAAI8M;YACJ9F,YAAYtG,IAAIlB,MAAM,CAACe,KAAK;YAC5BmJ,SAAS,CAAC,IAAI,EAAErK,KAAKK,GAAG,IAAI;YAC5BY,YAAYI,IAAIlB,MAAM,CAACS,QAAQ,CAACK,UAAU,IAAII,IAAIlB,MAAM,CAACe,KAAK;YAC9DwM,iBAAiBrM,IAAIV,EAAE;YACvB0K,qBAAqBJ;YACrB0C,YAAYrN,KAAKsN,KAAK,CAAC,OAAOtN,KAAKC,MAAM,KAAK;YAC9CgI,mBAAmB;YACnBzH,YAAY,IAAId;QAClB;QAEA,kBAAkB;QAClB,IAAI,CAAC,IAAI,CAAC2J,aAAa,CAACkE,GAAG,CAACxM,IAAIlB,MAAM,CAACe,KAAK,GAAG;YAC7C,IAAI,CAACyI,aAAa,CAAC/G,GAAG,CAACvB,IAAIlB,MAAM,CAACe,KAAK,EAAE,EAAE;QAC7C;QACA,IAAI,CAACyI,aAAa,CAACvG,GAAG,CAAC/B,IAAIlB,MAAM,CAACe,KAAK,EAAGqE,IAAI,CAACyC;QAE/C,OAAOA;IACT;IAEQtE,oBAAoBrC,GAAgB,EAAEpB,KAAU,EAAQ;QAC9DoB,IAAIC,MAAM,GAAG;QACbD,IAAIpB,KAAK,GAAG;YACV2I,SAAS3I,iBAAiBoD,QAAQpD,MAAM2I,OAAO,GAAG;YAClDkF,OAAO7N,iBAAiBoD,QAAQpD,MAAM6N,KAAK,GAAG9I;YAC9CjF,WAAW,IAAIC;QACjB;QAEA,IAAI,CAACuD,cAAc,CAAClC,KAAK,SAAS,CAAC,iBAAiB,EAAEA,IAAIpB,KAAK,CAAC2I,OAAO,EAAE;QAEzE,IAAI,CAAC9I,IAAI,CAAC,aAAa;YACrBM,OAAOiB,IAAIV,EAAE;YACbV,OAAOoB,IAAIpB,KAAK;QAClB;IACF;IAEQsD,eAAelC,GAAgB,EAAE0M,KAA2B,EAAEnF,OAAe,EAAEhI,QAAc,EAAQ;QAC3GS,IAAIkB,IAAI,CAACgD,IAAI,CAAC;YACZxF,WAAW,IAAIC;YACf+N;YACAnF;YACAhI;QACF;QAEA,oDAAoD;QACpD,IAAIS,IAAIkB,IAAI,CAAC6B,MAAM,GAAG,MAAM;YAC1B/C,IAAIkB,IAAI,GAAGlB,IAAIkB,IAAI,CAACuK,KAAK,CAAC,CAAC;QAC7B;IACF;IAEA,MAAclI,wBAAwBL,QAAgB,EAAEpE,MAAW,EAAmB;QACpF,6CAA6C;QAC7C,MAAM6N,kBAAkB,CAAC;;WAElB,EAAEzJ,SAAS;;mDAE6B,CAAC;QAEhD,IAAI;YACF,MAAMM,WAAW,MAAMC,sBAAU,CAACC,QAAQ,CAACiJ,iBAAiBhJ,WAAW;gBACrEC,aAAa9E,OAAO8E,WAAW;gBAC/BC,aAAa5E,KAAK0M,GAAG,CAAC,KAAK7M,OAAOgF,UAAU;YAC9C;YACA,OAAON,SAASA,QAAQ,CAACoJ,IAAI;QAC/B,EAAE,OAAOhO,OAAO;YACdR,QAAQiG,IAAI,CAAC,wDAAwDzF;YACrE,OAAOsE;QACT;IACF;IAEA,MAAce,kBAAkBX,MAAc,EAAEE,QAAgB,EAAmB;QACjF,oEAAoE;QACpE,IAAIqJ,QAAQ;QAEZ,wBAAwB;QACxB,IAAIrJ,SAAST,MAAM,GAAG,MAAMS,SAAST,MAAM,GAAG,MAAM8J,SAAS;QAE7D,yCAAyC;QACzC,IAAIrJ,SAAS2E,QAAQ,CAAC,QAAQ3E,SAASsJ,KAAK,CAAC,KAAK/J,MAAM,GAAG,GAAG8J,SAAS;QAEvE,yCAAyC;QACzC,MAAME,cAAczJ,OAAO0J,WAAW,GAAGF,KAAK,CAAC;QAC/C,MAAMG,gBAAgBzJ,SAASwJ,WAAW,GAAGF,KAAK,CAAC;QACnD,MAAMI,UAAUH,YAAYhF,MAAM,CAACoF,CAAAA,OAAQF,cAAc9E,QAAQ,CAACgF,OAAOpK,MAAM;QAC/E8J,SAAS5N,KAAK0M,GAAG,CAAC,KAAKuB,UAAUH,YAAYhK,MAAM;QAEnD,OAAO9D,KAAK0M,GAAG,CAAC,KAAKkB;IACvB;IAEA,MAActI,wBAAwB1C,IAAW,EAAgB;QAC/D,IAAIA,KAAKkB,MAAM,KAAK,GAAG;YACrB,OAAO;gBAAEqK,iBAAiB;gBAAGC,iBAAiB;gBAAGC,iBAAiB;gBAAGC,cAAc;YAAE;QACvF;QAEA,IAAIC,iBAAiB;QACrB,IAAIC,iBAAiB;QACrB,IAAIC,cAAc;QAElB,KAAK,MAAMC,QAAQ9L,KAAM;YACvB2L,kBAAkB,MAAM,IAAI,CAACvJ,iBAAiB,CAAC0J,KAAKrK,MAAM,EAAEqK,KAAKnK,QAAQ;YACzEiK,kBAAkB,MAAM,IAAI,CAACxJ,iBAAiB,CAAC0J,KAAKrK,MAAM,EAAEqK,KAAKnK,QAAQ;YACzEkK,eAAe,KAAK,wCAAwC;QAC9D;QAEA,uDAAuD;QACvD,MAAME,gBAAgB,IAAIC,IAAIhM,KAAKiM,GAAG,CAACH,CAAAA,OAAQA,KAAKrK,MAAM,CAAC0J,WAAW,KAAKe,IAAI;QAC/E,MAAMC,iBAAiBJ,gBAAgB/L,KAAKkB,MAAM;QAElD,OAAO;YACLqK,iBAAiBI,iBAAiB3L,KAAKkB,MAAM;YAC7CsK,iBAAiBI,iBAAiB5L,KAAKkB,MAAM;YAC7CuK,iBAAiBU;YACjBT,cAAcG,cAAc7L,KAAKkB,MAAM;QACzC;IACF;IAEA,MAAc2B,kBAAkB7C,IAAW,EAAE2C,UAAkB,EAAEyJ,MAAc,EAAiB;QAC9F,qEAAqE;QACrE7P,QAAQC,GAAG,CAAC,CAAC,OAAO,EAAEwD,KAAKkB,MAAM,CAAC,YAAY,EAAEyB,WAAW,IAAI,EAAEyJ,OAAO,OAAO,CAAC;IAClF;IAEA,MAAc3I,uBAAuBR,SAAiB,EAAEM,OAAe,EAAgB;QACrF,uCAAuC;QACvC,OAAO;YACL8I,cAAc9I;YACdtE,SAAS;gBACPgL,UAAU,OAAO7M,KAAKC,MAAM,KAAK;gBACjC6M,UAAU,OAAO9M,KAAKC,MAAM,KAAK;gBACjCiP,YAAY,OAAOlP,KAAKC,MAAM,KAAK;YACrC;YACAkP,cAAc;YACdC,iBAAiB1P,KAAKK,GAAG;QAC3B;IACF;IAEA,MAAc6G,sBAAsBf,SAAiB,EAAEY,MAAc,EAAmB;QACtF,sCAAsC;QACtC,OAAO,MAAMzG,KAAKC,MAAM,KAAK;IAC/B;IAEQ6G,sBAAsBjF,OAA+B,EAAU;QACrE,MAAMwN,UAAU;YACdxC,UAAU;YACVC,UAAU;YACVoC,YAAY;YACZlC,gBAAgB,CAAC;YACjBC,cAAc,CAAC,IAAO,mCAAmC;QAC3D;QAEA,IAAIW,QAAQ;QACZ,IAAI0B,cAAc;QAElBhJ,OAAOC,OAAO,CAAC1E,SAAS2E,OAAO,CAAC,CAAC,CAACC,QAAQC,MAAM;YAC9C,MAAM6I,SAASF,OAAO,CAAC5I,OAAO,IAAI;YAClCmH,SAASlH,QAAQ6I;YACjBD,eAAetP,KAAKwP,GAAG,CAACD;QAC1B;QAEA,OAAOD,cAAc,IAAItP,KAAKoL,GAAG,CAAC,GAAGpL,KAAK0M,GAAG,CAAC,GAAGkB,QAAQ0B,gBAAgB;IAC3E;IAEA,MAActI,mCACZnB,SAAiB,EACjBhE,OAA+B,EAC/BoE,gBAAqC,EAClB;QACnB,MAAMc,kBAA4B,EAAE;QAEpC,IAAIlF,QAAQgL,QAAQ,GAAG,KAAK;YAC1B9F,gBAAgB9B,IAAI,CAAC;QACvB;QAEA,IAAIpD,QAAQmL,cAAc,GAAG,KAAK;YAChCjG,gBAAgB9B,IAAI,CAAC;QACvB;QAEA,IAAIpD,QAAQoL,YAAY,GAAG,MAAM;YAC/BlG,gBAAgB9B,IAAI,CAAC;QACvB;QAEA,OAAO8B;IACT;IAEA,MAAcY,gBAAgBwF,SAAiB,EAAwC;QACrF,KAAK,MAAMsC,YAAY,IAAI,CAACpG,aAAa,CAACR,MAAM,GAAI;YAClD,MAAMkB,UAAU0F,SAASC,IAAI,CAACC,CAAAA,IAAKA,EAAEtP,EAAE,KAAK8M;YAC5C,IAAIpD,SAAS,OAAOA;QACtB;QACA,OAAO;IACT;IAEA,MAAc5B,mBAAmB4B,OAA6B,EAAiB;QAC7E,+CAA+C;QAC/C5K,QAAQC,GAAG,CAAC,CAAC,uBAAuB,EAAE2K,QAAQ1J,EAAE,EAAE;IACpD;IAEA,MAAcwH,eAAekC,OAA6B,EAAElK,MAAW,EAAgB;QACrF,8BAA8B;QAC9B,OAAO;YAAEmB,QAAQ;YAAW4O,UAAU,CAAC,mCAAmC,CAAC;QAAC;IAC9E;IAEA,MAAc9H,oBAAoBiC,OAA6B,EAAElK,MAAW,EAAgB;QAC1F,mCAAmC;QACnC,OAAO;YAAEmB,QAAQ;YAAW4O,UAAU,CAAC,8BAA8B,EAAE7F,QAAQ1C,UAAU,EAAE;QAAC;IAC9F;IAEA,MAAcU,cAAcgC,OAA6B,EAAElK,MAAW,EAAgB;QACpF,6BAA6B;QAC7B,OAAO;YAAEmB,QAAQ;YAAW4O,UAAU,CAAC,kCAAkC,CAAC;QAAC;IAC7E;IAEA,MAAc5H,cAAc+B,OAA6B,EAAElK,MAAW,EAAgB;QACpF,6BAA6B;QAC7B,OAAO;YAAEmB,QAAQ;YAAW4O,UAAU,CAAC,sCAAsC,EAAE7F,QAAQ1J,EAAE,EAAE;QAAC;IAC9F;IAp5BA,aAAc;QACZ,KAAK,IAVP,uBAAQkC,cAAR,KAAA,IACA,uBAAQsN,sBAAR,KAAA,IACA,uBAAQC,sBAAR,KAAA,IACA,uBAAQzN,cAAR,KAAA,IACA,uBAAQgH,iBAAR,KAAA,IACA,uBAAQ0G,iBAAR,KAAA,IACA,uBAAQC,sBAAR,KAAA,IACA,uBAAQ9Q,iBAAgB;QAItB,IAAI,CAACqD,UAAU,GAAG0N,sBAAU,CAACC,WAAW;QACxC,IAAI,CAAC7N,UAAU,GAAG,IAAI+G;QACtB,IAAI,CAACC,aAAa,GAAG,IAAID;QAEzB,sBAAsB;QACtB,IAAI,CAACyG,kBAAkB,GAAG,IAAIM,sCAAkB,CAAC;YAC/CC,qBAAqB;YACrBC,mBAAmB;YACnBC,iBAAiB;YACjBC,cAAc;YACdC,gBAAgB;YAChBC,iBAAiB;gBACfC,cAAc;gBACdC,WAAW;gBACXC,aAAa;YACf;QACF;QACA,IAAI,CAACd,kBAAkB,GAAG,IAAIe,sCAAkB;QAEhD,oBAAoB;QACpB,IAAI,CAACd,aAAa,GAAG,IAAIe,kBAAQ,CAAC;YAChC1F,KAAK;YACL2F,KAAK,OAAO,KAAK,KAAK,EAAE,UAAU;QACpC;QAEA,IAAI,CAACf,kBAAkB,GAAG,IAAI5G;IAChC;AAy3BF;AAGO,MAAMrK,sBAAsB,IAAID"}