# Optimized Docker Compose for GHCR Images
# Features: Multi-architecture support, health checks, resource optimization, monitoring

version: '3.8'

# Global configuration
x-common-variables: &common-variables
  NODE_ENV: ${NODE_ENV:-production}
  LOG_LEVEL: ${LOG_LEVEL:-info}
  OPTIMIZATION_ENABLED: ${OPTIMIZATION_ENABLED:-true}

x-common-labels: &common-labels
  org.opencontainers.image.vendor: "Prompt Card System"
  org.opencontainers.image.licenses: "MIT"
  org.opencontainers.image.source: "https://github.com/tbowman01/prompt-card-system"

x-healthcheck-defaults: &healthcheck-defaults
  interval: 15s
  timeout: 5s
  retries: 3
  start_period: 30s

x-restart-policy: &restart-policy
  restart: unless-stopped

x-logging-config: &logging-config
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

networks:
  prompt-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
    labels:
      <<: *common-labels
      network.purpose: "prompt-card-system"

services:
  # ===== FRONTEND SERVICE =====
  frontend:
    image: ${FRONTEND_IMAGE:-ghcr.io/tbowman01/prompt-card-frontend:latest}
    container_name: prompt-frontend
    labels:
      <<: *common-labels
      service.name: "frontend"
      service.type: "web"
      traefik.enable: "true"
      traefik.http.routers.frontend.rule: "Host(`${FRONTEND_HOST:-localhost}`)"
      traefik.http.services.frontend.loadbalancer.server.port: "3000"
    environment:
      <<: *common-variables
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:3001}
      NEXT_PUBLIC_WS_URL: ${NEXT_PUBLIC_WS_URL:-ws://localhost:3001}
      NEXT_PUBLIC_AUTH_URL: ${NEXT_PUBLIC_AUTH_URL:-http://localhost:8005}
      NEXT_PUBLIC_APP_VERSION: ${APP_VERSION:-latest}
      # Performance optimizations
      NEXT_TELEMETRY_DISABLED: 1
      NODE_OPTIONS: "--max-old-space-size=1024"
      # Security headers
      NEXT_PUBLIC_CSP_ENABLED: ${CSP_ENABLED:-true}
      NEXT_PUBLIC_SECURE_HEADERS: ${SECURE_HEADERS:-true}
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    volumes:
      - frontend_cache:/app/.next/cache
      - frontend_logs:/app/logs
    networks:
      - prompt-network
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      start_period: 45s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    <<: *restart-policy
    <<: *logging-config

  # ===== BACKEND SERVICE =====
  backend:
    image: ${BACKEND_IMAGE:-ghcr.io/tbowman01/prompt-card-backend:latest}
    container_name: prompt-backend
    labels:
      <<: *common-labels
      service.name: "backend"
      service.type: "api"
      traefik.enable: "true"
      traefik.http.routers.backend.rule: "Host(`${API_HOST:-localhost}`) && PathPrefix(`/api`)"
      traefik.http.services.backend.loadbalancer.server.port: "3001"
    environment:
      <<: *common-variables
      PORT: 3001
      # Database configuration
      DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/prompt_card_system}
      DATABASE_POOL_SIZE: ${DATABASE_POOL_SIZE:-20}
      DATABASE_CONNECTION_TIMEOUT: ${DATABASE_CONNECTION_TIMEOUT:-30000}
      # Redis configuration  
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
      REDIS_CONNECTION_POOL_SIZE: ${REDIS_CONNECTION_POOL_SIZE:-10}
      # Security
      JWT_SECRET: ${JWT_SECRET}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      CORS_ORIGIN: ${CORS_ORIGIN:-http://localhost:3000}
      # Performance optimizations
      NODE_OPTIONS: "--max-old-space-size=2048 --enable-source-maps=false"
      UV_THREADPOOL_SIZE: ${UV_THREADPOOL_SIZE:-16}
      # API configuration
      API_RATE_LIMIT: ${API_RATE_LIMIT:-1000}
      API_BURST_LIMIT: ${API_BURST_LIMIT:-100}
      # Monitoring
      MONITORING_ENABLED: ${MONITORING_ENABLED:-true}
      METRICS_PORT: ${METRICS_PORT:-9090}
      # LLM integration
      OLLAMA_HOST: ${OLLAMA_HOST:-http://ollama:11434}
      OLLAMA_TIMEOUT: ${OLLAMA_TIMEOUT:-60000}
    ports:
      - "${BACKEND_PORT:-3001}:3001"
      - "${METRICS_PORT:-9090}:9090"
    volumes:
      - backend_data:/app/data
      - backend_uploads:/app/uploads
      - backend_logs:/app/logs
      - backend_cache:/app/cache
    networks:
      - prompt-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3001/health"]
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: '1.5'
        reservations:
          memory: 1.5G
          cpus: '0.75'
    <<: *restart-policy
    <<: *logging-config

  # ===== AUTH SERVICE =====
  auth:
    image: ${AUTH_IMAGE:-ghcr.io/tbowman01/prompt-card-auth:latest}
    container_name: prompt-auth
    labels:
      <<: *common-labels
      service.name: "auth"
      service.type: "security"
      traefik.enable: "true"
      traefik.http.routers.auth.rule: "Host(`${AUTH_HOST:-localhost}`) && PathPrefix(`/auth`)"
      traefik.http.services.auth.loadbalancer.server.port: "8005"
    environment:
      <<: *common-variables
      PORT: 8005
      # Database
      DATABASE_URL: ${AUTH_DATABASE_URL:-postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/prompt_card_system}
      # Security
      JWT_SECRET: ${JWT_SECRET}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      SESSION_SECRET: ${SESSION_SECRET}
      # Auth providers
      OAUTH_GOOGLE_CLIENT_ID: ${OAUTH_GOOGLE_CLIENT_ID}
      OAUTH_GOOGLE_CLIENT_SECRET: ${OAUTH_GOOGLE_CLIENT_SECRET}
      OAUTH_GITHUB_CLIENT_ID: ${OAUTH_GITHUB_CLIENT_ID}
      OAUTH_GITHUB_CLIENT_SECRET: ${OAUTH_GITHUB_CLIENT_SECRET}
      # Session configuration
      SESSION_TIMEOUT: ${SESSION_TIMEOUT:-86400}
      TOKEN_EXPIRY: ${TOKEN_EXPIRY:-3600}
      REFRESH_TOKEN_EXPIRY: ${REFRESH_TOKEN_EXPIRY:-604800}
      # Performance
      NODE_OPTIONS: "--max-old-space-size=1024"
      # Redis for sessions
      REDIS_URL: ${REDIS_URL:-redis://redis:6379}
    ports:
      - "${AUTH_PORT:-8005}:8005"
    volumes:
      - auth_data:/app/data
      - auth_logs:/app/logs
    networks:
      - prompt-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      <<: *healthcheck-defaults
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8005/auth/health"]
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    <<: *restart-policy
    <<: *logging-config

  # ===== OLLAMA LLM SERVICE =====
  ollama:
    image: ${OLLAMA_IMAGE:-ghcr.io/tbowman01/prompt-card-ollama:latest}
    container_name: prompt-ollama
    labels:
      <<: *common-labels
      service.name: "ollama"
      service.type: "ai"
    environment:
      OLLAMA_HOST: 0.0.0.0
      OLLAMA_ORIGINS: ${OLLAMA_ORIGINS:-*}
      OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-4}
      OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS:-2}
      OLLAMA_FLASH_ATTENTION: ${OLLAMA_FLASH_ATTENTION:-true}
      # GPU configuration
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-all}
      # Model configuration
      DEFAULT_MODEL: ${DEFAULT_MODEL:-phi4-mini-reasoning:3.8b}
      PRELOAD_MODELS: ${PRELOAD_MODELS:-phi4-mini-reasoning:3.8b,llama3.2:1b}
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_models:/root/.ollama
      - ollama_cache:/tmp/ollama
    networks:
      - prompt-network
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-8G}
          cpus: '4.0'
        reservations:
          memory: ${OLLAMA_MEMORY_RESERVATION:-4G}
          cpus: '2.0'
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    <<: *restart-policy
    <<: *logging-config

  # ===== POSTGRESQL DATABASE =====
  postgres:
    image: postgres:15-alpine
    container_name: prompt-postgres
    labels:
      <<: *common-labels
      service.name: "postgres"
      service.type: "database"
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-prompt_card_system}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
      PGDATA: /var/lib/postgresql/data/pgdata
      # Performance tuning
      POSTGRES_SHARED_BUFFERS: ${POSTGRES_SHARED_BUFFERS:-256MB}
      POSTGRES_EFFECTIVE_CACHE_SIZE: ${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      POSTGRES_WORK_MEM: ${POSTGRES_WORK_MEM:-4MB}
      POSTGRES_MAINTENANCE_WORK_MEM: ${POSTGRES_MAINTENANCE_WORK_MEM:-64MB}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - postgres_logs:/var/log/postgresql
    networks:
      - prompt-network
    command: >
      postgres
      -c max_connections=${POSTGRES_MAX_CONNECTIONS:-100}
      -c shared_buffers=${POSTGRES_SHARED_BUFFERS:-256MB}
      -c effective_cache_size=${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      -c maintenance_work_mem=${POSTGRES_MAINTENANCE_WORK_MEM:-64MB}
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=${POSTGRES_WORK_MEM:-4MB}
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c log_statement=all
      -c log_duration=on
      -c log_line_prefix='%t [%p]: '
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-prompt_card_system}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    <<: *restart-policy
    <<: *logging-config

  # ===== REDIS CACHE =====
  redis:
    image: redis:7.2-alpine
    container_name: prompt-redis
    labels:
      <<: *common-labels
      service.name: "redis"
      service.type: "cache"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      REDIS_MAXMEMORY: ${REDIS_MAXMEMORY:-1gb}
      REDIS_MAXMEMORY_POLICY: ${REDIS_MAXMEMORY_POLICY:-allkeys-lru}
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
      - redis_conf:/usr/local/etc/redis
      - redis_logs:/var/log/redis
    networks:
      - prompt-network
    command: >
      sh -c "
      echo 'save 900 1' > /usr/local/etc/redis/redis.conf &&
      echo 'save 300 10' >> /usr/local/etc/redis/redis.conf &&
      echo 'save 60 10000' >> /usr/local/etc/redis/redis.conf &&
      echo 'maxmemory ${REDIS_MAXMEMORY:-1gb}' >> /usr/local/etc/redis/redis.conf &&
      echo 'maxmemory-policy ${REDIS_MAXMEMORY_POLICY:-allkeys-lru}' >> /usr/local/etc/redis/redis.conf &&
      echo 'tcp-keepalive 60' >> /usr/local/etc/redis/redis.conf &&
      echo 'timeout 300' >> /usr/local/etc/redis/redis.conf &&
      echo 'appendonly yes' >> /usr/local/etc/redis/redis.conf &&
      echo 'appendfsync everysec' >> /usr/local/etc/redis/redis.conf &&
      echo 'auto-aof-rewrite-percentage 100' >> /usr/local/etc/redis/redis.conf &&
      echo 'auto-aof-rewrite-min-size 64mb' >> /usr/local/etc/redis/redis.conf &&
      if [ -n '${REDIS_PASSWORD}' ]; then
        echo 'requirepass ${REDIS_PASSWORD}' >> /usr/local/etc/redis/redis.conf;
        redis-server /usr/local/etc/redis/redis.conf;
      else
        redis-server /usr/local/etc/redis/redis.conf;
      fi
      "
    healthcheck:
      test: |
        if [ -n "${REDIS_PASSWORD}" ]; then
          redis-cli -a "${REDIS_PASSWORD}" ping
        else
          redis-cli ping
        fi
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    <<: *restart-policy
    <<: *logging-config

  # ===== MONITORING: PROMETHEUS =====
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prompt-prometheus
    labels:
      <<: *common-labels
      service.name: "prometheus"
      service.type: "monitoring"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--storage.tsdb.wal-compression'
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus:ro
      - prometheus_data:/prometheus
      - prometheus_config:/etc/prometheus/config
    networks:
      - prompt-network
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    <<: *restart-policy
    <<: *logging-config

  # ===== MONITORING: GRAFANA =====
  grafana:
    image: grafana/grafana:10.2.0
    container_name: prompt-grafana
    labels:
      <<: *common-labels
      service.name: "grafana"
      service.type: "monitoring"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_USERS_ALLOW_ORG_CREATE: false
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_INSTALL_PLUGINS: redis-datasource,prometheus
      # Performance settings
      GF_DATABASE_WAL: true
      GF_DATABASE_CACHE_MODE: shared
      GF_LOG_LEVEL: ${GRAFANA_LOG_LEVEL:-info}
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_logs:/var/log/grafana
      - grafana_plugins:/var/lib/grafana/plugins
      - ./monitoring/grafana:/etc/grafana/provisioning:ro
    networks:
      - prompt-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 256M
          cpus: '0.1'
    <<: *restart-policy
    <<: *logging-config

# ===== VOLUMES =====
volumes:
  # Application data
  backend_data:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "backend-data"
  backend_uploads:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "backend-uploads"
  backend_cache:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "backend-cache"
  auth_data:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "auth-data"
  frontend_cache:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "frontend-cache"
  
  # Database volumes
  postgres_data:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "postgres-data"
  postgres_backups:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "postgres-backups"
  
  # Cache volumes
  redis_data:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "redis-data"
  redis_conf:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "redis-config"
  
  # AI model volumes
  ollama_models:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "ollama-models"
  ollama_cache:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "ollama-cache"
  
  # Monitoring volumes
  prometheus_data:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "prometheus-data"
  prometheus_config:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "prometheus-config"
  grafana_data:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "grafana-data"
  grafana_plugins:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "grafana-plugins"
  
  # Log volumes
  backend_logs:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "backend-logs"
  auth_logs:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "auth-logs"
  postgres_logs:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "postgres-logs"
  redis_logs:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "redis-logs"
  grafana_logs:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "grafana-logs"
  frontend_logs:
    driver: local
    labels:
      <<: *common-labels
      volume.purpose: "frontend-logs"