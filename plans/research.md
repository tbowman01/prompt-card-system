# Prompt CardSystem MVP – 30-Day Project Plan

**Team & Scope:** A small team (1–3 developers) will build an MVP for a Prompt Card system in \~4 weeks. The stack includes a Next.js frontend, Node.js (Express) backend with SQLite, local LLM via Ollama (Docker), and prompt testing with Promptfoo. The plan is organized into weekly milestones with specific focus, tasks, responsibilities, and dependencies. Each week aims for steady progress on core components, assuming parallel work where possible (frontend vs. backend). If fewer developers are available, tasks are handled sequentially by the same person.

## Week 1: Project Setup & Architecture

*Focus:* Establish the development environment, core architecture, and scaffolding for frontend and backend. Define data models and ensure all services can communicate.

**Tasks:**

* **Planning & Design:** Hold a kickoff meeting to confirm requirements and system design. Decide on project structure (e.g. monorepo vs. separate apps) and how Next.js frontend will interact with the Express API (REST endpoints, CORS policy, etc.). Draft the data model for “Prompt Card” (e.g. fields for title, prompt text, etc.) and associated “Test Case” entities.
* **Frontend Setup:** Initialize a Next.js app (using `create-next-app` with TypeScript). Set up a clean project structure for pages and components. Implement a placeholder homepage (e.g. “Prompt Card MVP”) to verify the app runs.
* **Backend Setup:** Initialize a Node.js project with Express. Set up basic Express server with a health-check route (e.g. GET `/api/health`) to ensure it starts without errors. Integrate SQLite (e.g. using an SQLite3 or better-sqlite3 library) and create a connection to a local database file. Design preliminary database schema for Prompt Cards and Test Cases tables.
* **Docker Compose:** Create a `docker-compose.yml` defining services for frontend, backend, and the Ollama LLM. For now, add the Next.js service and Express API service, each in its own container, and link them. Mount volumes for code (to enable hot-reload in development) and expose necessary ports (Next.js default 3000, API e.g. 3001). Include a volume or mechanism for the SQLite data file so data persists. Add a placeholder for the Ollama service (using the official Ollama Docker image) – this will be configured in a later week once the model is chosen.
* **Connectivity Test:** Implement a simple API endpoint (e.g. GET `/api/ping`) in Express and call it from the Next.js frontend using `fetch` to verify end-to-end communication. Handle CORS or proxy settings as needed so the frontend can reach the backend. Confirm that the frontend displays the response (e.g. “pong”) from the API.
* **Version Control:** Set up a git repository and make an initial commit of the scaffold. Ensure all team members can run the app via Docker Compose (or locally if preferred during dev) and update README with basic setup steps.

**Responsibilities:**

* *Frontend Dev:* Lead the Next.js app initialization and ensure the UI can communicate with the API.
* *Backend Dev:* Handle Express server setup, database initialization, and define the data schema.
* *DevOps/Full-Stack:* Configure Docker Compose and environment variables. Ensure that services (frontend, backend, DB) run together. (In a smaller team, one person may perform multiple roles.) The whole team aligns on architecture decisions and resolves any setup issues together.

**Dependencies:**

* Development tools installed (Node.js 18+, Docker/Docker Compose).
* Agreement on data schema and API design (prompt card fields, test case structure) by end of week. This guides the implementation in Week 2.
* No external libraries beyond chosen stack are strictly required yet, but planning for Promptfoo and Ollama integration is ongoing (e.g. ensuring the Ollama service can be included later).

## Week 2: Prompt Cards CRUD & YAML Config Support

*Focus:* Implement core functionality for managing Prompt Cards and their test cases. Develop the UI forms and API endpoints for creating, reading, updating, and deleting prompt cards. Begin integrating YAML parsing to import/export prompt configurations (in preparation for Promptfoo tests).

**Tasks:**

* **Prompt Card Model & CRUD API:** Implement the SQLite database schema for prompt cards (e.g. `PromptCard` table with columns: id, title, prompt text, description, etc.) and test cases (e.g. `TestCase` table with id, promptCardId, inputVars, expectedOutput or assertion criteria). Set up Express routes for CRUD: GET `/api/prompt-cards` (list all), GET `/api/prompt-cards/:id` (fetch one, including its test cases), POST (create new prompt card), PUT/PATCH (edit prompt card), DELETE (remove). The POST/PUT handlers should also handle any included test case data.
* **Frontend UI – Prompt Cards:** Build pages in Next.js for Prompt Cards. This includes a listing page (show all saved prompt cards with basic info), a form page to create a new prompt card, and an edit page to modify an existing one. The form should capture at least the prompt title and the prompt template text (which may include placeholders for variables, e.g. `{{variable}}`). Use React state to manage form inputs and use Next.js API routes or direct calls to the Express API to save data. After creating or updating, update the list view accordingly. Keep the UI simple and developer-friendly (no fancy styling needed, but ensure form elements are clear and labeled). Keep paragraphs short and clearly separate form sections.
* **Test Case Management:** Extend the data model and UI to handle test cases for each prompt card. For MVP, allow adding multiple test cases to a prompt card. Each test case at minimum should capture the input variables to substitute into the prompt. Implement a simple UI within the Prompt Card form or detail page: e.g. a section “Test Cases” where a user can add entries. For each test case, provide a way to input variable values (this could be a JSON or YAML snippet of `{key: value}` pairs, or a small dynamic form that lets the user add key-value pairs). For now, storing the variables as a JSON string in the database is acceptable for simplicity. Optionally, allow an “expected output” or assertion field for each test (even if just free text) – this can later be used to verify correctness.
* **Backend for Test Cases:** Create Express endpoints to add or update test cases (possibly handled via the Prompt Card endpoints: e.g. including test case data in the POST/PUT for prompt cards, or separate routes like POST `/api/prompt-cards/:id/test-cases`). Ensure that when a prompt card is fetched, its test cases are also returned (to display in the UI). Implement data validation (e.g. each test case must include all variables that the prompt template expects).
* **YAML Parsing Integration:** Introduce a YAML library (e.g. js-yaml) into the backend. The goal is to support importing and exporting prompt configurations in Promptfoo’s YAML format. Promptfoo uses a YAML config to define prompts, providers, and tests, so our app will convert our data to this format. Implement a utility function that can take a Prompt Card (with its test cases) and generate a YAML string matching promptfoo’s schema (e.g. with keys: `prompts`, `providers`, `tests`, etc.). Also support the reverse: parse a given YAML to extract prompt(s) and tests – for example, allow the user to import an existing promptfoo YAML config file. This might involve mapping YAML fields to our PromptCard model (e.g. `prompts` content to prompt text, each `tests` entry to a TestCase’s vars and assertions).
* **Example Data:** Create one or two example prompt cards (either via the UI or directly in the database) to verify everything so far. For instance, an example prompt card could be “Translate English to French” with a prompt template “Translate `{{input}}` to French.” Include a couple of test cases (one with input="Hello world", expected output containing "Bonjour"; another with input="How are you?", expected output containing "Comment ça va"). These examples will also serve as a basis for testing the YAML import/export.
* **Testing & Refinement:** Manually test the end-to-end flow: create a prompt card via UI, add a test case, save to DB, fetch and edit it. Ensure data persists in SQLite. Test the YAML conversion by exporting the example prompt card to YAML and comparing with promptfoo’s expected format (the YAML should list the prompt and the test case variables). If possible, run `promptfoo eval` on the exported YAML to ensure it’s valid (this requires having a provider configured, which we will fully integrate in Week 3). Adjust parsing logic based on any issues.

**Responsibilities:**

* *Backend Dev:* Implements the data models, migration (if any) for SQLite, and all Express API endpoints for prompt cards and test cases. Also leads development of YAML import/export utilities.
* *Frontend Dev:* Builds the Next.js pages and components for listing prompt cards and editing/creating them, including test case sub-forms. Integrates frontend with the backend API (using fetch or axios) and handles user input validation (e.g. required fields).
* *Team Collaboration:* Both frontend and backend developers work closely to ensure the data contract (API request/response shapes) align. For example, agree on the JSON format for test case data in API. If only one developer, prioritize backend API first, then the UI, testing each in turn. Frequent communication is key to get CRUD functionality working smoothly by week’s end.

**Dependencies:**

* Completion of Week 1 setup (functional Next.js and Express apps) is required to start building features. The database and server should be running via Docker or locally as decided.
* A YAML parsing library (e.g. **js-yaml**) is needed as a new dependency for the backend. Ensure this is added to the project and tested.
* Understanding of Promptfoo’s YAML format is necessary to implement correct parsing. The Promptfoo documentation provides guidance (e.g. showing `prompts`, `providers`, `tests` sections). No actual LLM or Promptfoo usage is required this week, but laying the groundwork for it (data structures and YAML compatibility) will make Week 3 easier.
* There are no external services needed yet (everything is local), so as long as the dev environment is consistent (Docker Compose up to date, etc.), the team can proceed without blockers.

## Week 3: LLM Integration & Prompt Testing

*Focus:* Integrate the Ollama LLM and Promptfoo testing into the application. Enable running tests against the prompt cards using the local LLM and display evaluation results in the UI.

**Tasks:**

* **Ollama LLM Setup:** Configure the Ollama service in Docker Compose to run a local LLM model. Select a suitable model (e.g. a smaller LLaMA 2 variant or any open-source model supported by Ollama) to download and serve. Ensure the Ollama container is up and accessible from the backend. By default, Ollama serves an API on `localhost:11434` – confirm the Docker network allows the Express backend to reach this (it may be accessible via `http://ollama:11434` if the service is named "ollama" in Compose). Set environment variables if needed (e.g. `OLLAMA_BASE_URL` for promptfoo to point to the correct host/port). Perform a quick manual test: from the Express container, send a curl or HTTP request to Ollama’s `/api/generate` endpoint with a simple prompt to verify it returns a completion.
* **Promptfoo Integration (Backend):** Install the Promptfoo npm package in the backend. Use Promptfoo’s Node API to run evaluations programmatically. In code, build a TestSuite configuration object from the Prompt Card and its Test Cases, then call `promptfoo.evaluate()` with this object. This object should include: the prompt template (or multiple prompts, but for MVP likely just one prompt or one prompt card at a time), the provider (configure to use the local Ollama model, e.g. provider id `ollama:chat:MODEL_NAME` appropriate to the chosen model), and the tests array (each with `vars` filled from the test case inputs, and possibly `assert` if we have expected output criteria). Ensure the provider is correctly set so that Promptfoo directs requests to Ollama. (Promptfoo supports Ollama as a provider, using the local API by default, so if the environment is configured, this should work.)
* **Test Execution Endpoint:** Implement an Express route to trigger evaluations, e.g. POST `/api/prompt-cards/:id/evaluate`. When called, the backend will retrieve the prompt card and its test cases from the database, construct the Promptfoo test suite config, and execute `evaluate()`. Because running the LLM can be time-consuming, handle this call asynchronously: possibly use an async route handler or even spawn a background job if needed (for MVP, synchronous handling with a loading spinner on the frontend is fine). Consider setting a reasonable concurrency limit – since we’re using a single local model, it might be wise to run tests serially to conserve RAM (Promptfoo can be configured to run with concurrency 1 to load one model at a time). Capture the results returned by Promptfoo. These results include the output of the model for each test, and whether any assertions passed or failed.
* **Results Handling:** Define how to represent the results. For each test case, relevant info includes: the input vars, the LLM’s output, and an indication of success/failure (if an assertion was provided). Decide if results will be stored in the database or just computed on the fly. For MVP, it’s acceptable to return the results directly to the client without persisting them (the user can re-run tests as needed). If storing, create a new table (e.g. `TestResult`) with a reference to the test case and fields for output, pass/fail, etc., but this is optional.
* **Frontend – Run Tests UI:** On the Prompt Card detail page, add a “Run Tests” button. When clicked, it should call the new evaluate API route. Provide user feedback that tests are running (disable the button and show a spinner or message like “Running tests…”). Once the API responds with results, display them clearly. For example, show a list or table: each test case on one row, with columns for input variables, the model’s output, and a ✅ or ❌ if an assertion check passed or failed. If no explicit expected output was set, you can simply show the output (and perhaps allow the user to judge quality manually). If we included an “expected output contains X” in test cases, highlight whether that was found. Ensure the output text is readable (consider using a `<pre>` or styled block for the LLM response).
* **Frontend – Results Visualization:** Make the results UI developer-friendly. Possibly format differences if multiple prompts or models were involved (for MVP likely one prompt, one model, so a simple list is fine). If Promptfoo returns a summary or metrics, you can display aggregate info (e.g. number of tests passed/failed). For example, if 3/5 tests passed, show a summary “Tests passed: 3/5”. If the Promptfoo evaluation supports a matrix view (multiple providers vs multiple prompts) and if we ever use it, ensure the UI can handle that (but initially, focus on single-provider scenario).
* **Integration Testing:** Test the full flow with the example prompt cards. Start the entire stack via Docker Compose. On the UI, create or use an example prompt card with some test cases. Click “Run Tests”. Verify that the backend calls Ollama (you should see the Ollama container logs receiving requests) and that the results come back. Check the outputs make sense (e.g. if the prompt was translation, does the output look like a French phrase?). If an assertion was defined (say expecting “Bonjour” in output), see that the result is marked passed ✅ if output contained it. Debug any issues: e.g. if the backend can’t reach Ollama, adjust network settings or use `OLLAMA_BASE_URL="http://ollama:11434"` in the Express env to target the container host. If outputs are empty or errors occur, handle errors gracefully (send error message to frontend).
* **Performance and Edge Cases:** Note that local LLM inference can be slow or memory heavy. If the model is large, running multiple tests might strain resources. Mitigate by running tests one at a time (Promptfoo’s concurrency option or just one provider/prompt at a time) and maybe limit the number of tests a user can add for now to a reasonable count. Also, ensure that the system doesn’t block indefinitely: use timeouts or at least inform the user if a test is taking unusually long. Since deployment is local, the user can monitor the Docker resource usage – document that if necessary.

**Responsibilities:**

* *DevOps/Backend:* Set up and verify the Ollama container and model. This includes managing model download and any configuration for networking. Also responsible for integrating the Promptfoo library into the backend and ensuring the evaluation function works correctly with the local LLM.
* *Backend Dev:* Implement the evaluate API route and the logic to transform Prompt Card data into the Promptfoo test suite format. Handle result parsing and error cases (e.g. catching exceptions if the LLM call fails). This developer will work closely with the DevOps role on LLM connectivity.
* *Frontend Dev:* Add the test execution UI (button, loading state, results display). Ensure the UX is clear when tests are running and when results are shown. This role will also parse and present any error messages returned (e.g. if the LLM is not available, display a user-friendly message to check the Docker setup).
* *Collaborative Testing:* All team members should participate in testing this feature, as it’s the core of the MVP. The frontend and backend devs will likely need to iterate together (tweaking the API and UI) to get a smooth experience. If the team is one person, allocate ample time this week for integration testing and troubleshooting.

**Dependencies:**

* Functional Prompt Cards CRUD from Week 2 (we need prompt data and test cases in place to test against). Any incomplete work from Week 2 (e.g. test case input UI) should be finished at the start of this week.
* Ollama and Promptfoo: Both must be installed/available. The plan relies on the Ollama Docker image (for the model) and the Promptfoo NPM package. Ensure the Docker host has enough resources (CPU/RAM) for the chosen model. If the model requires acceptance of a license (some LLaMA models do), handle that beforehand or choose a permissive model to avoid delays.
* Promptfoo configuration knowledge: The team should be familiar with specifying providers and test cases. E.g., using provider strings for Ollama (like `ollama:chat:llama2:7b`) and how to structure assertions if used. The Promptfoo Node API documentation is a reference for using `evaluate()`. Keep the prompt syntax consistent with how variables are defined in test cases (e.g. using `{{varName}}` placeholders in the prompt template, which Promptfoo will replace with `vars` values).
* Docker Compose should be updated this week to ensure the backend service can talk to the Ollama service. This might involve setting environment variables in the backend container (as noted) or linking networks. The team must coordinate on these configuration changes.
* There’s a risk that unexpected issues (like networking or model performance) could arise – if so, be prepared to pivot (e.g. if Ollama model is too slow, perhaps limit to very simple tests or consider a smaller model). Allocate some buffer time for solving such issues.

## Week 4: Polish, Documentation & Delivery

*Focus:* Finalize the MVP with improvements, ensure all features work together smoothly, add example content, and prepare documentation. This week is about fixing any remaining bugs, improving usability, and delivering a usable product with guidance for users.

**Tasks:**

* **UI/UX Refinement:** Clean up the frontend for a better user experience. This includes organizing the layout (for example, use a simple navigation or menu if needed to switch between the list and create pages), adding labels or help text where appropriate (e.g. explain that `{{variable}}` notation can be used in prompt text, or how to add multiple test cases). Ensure form validations are in place – for instance, prevent creating a prompt card without a prompt text, or running tests if no test cases exist (show a warning in that case). If styling is bare-bones, consider using a minimal component library or custom CSS to make the app look presentable (but don’t spend too long – basic styling is fine for MVP). Test the interface for responsiveness (if likely to be used on different screen sizes) and fix any obvious issues.
* **YAML Import/Export Feature:** Complete the YAML integration started in Week 2. If not already done, add UI controls for it. For example, on the Prompt Cards page or a dedicated settings page, allow the user to **import** a YAML configuration: they can paste YAML text or upload a `.yaml` file. Parse this on the backend to create a new Prompt Card with test cases (or multiple prompt cards if the YAML contains an array of prompts). Conversely, provide an **export** option: perhaps a button “Export YAML” on a prompt card that generates the promptfoo YAML (including the prompt, provider (set to Ollama by default), and tests) and either displays it or downloads it as a file. This caters to advanced users who might want to edit the config by hand or use it outside the app. Make sure to handle errors (e.g. invalid YAML input should return a clear error message). By implementing this, the system aligns with Promptfoo’s format and fulfills the “include YAML parsing” requirement – effectively bridging our UI with Promptfoo’s config files.
* **Documentation & Example Prompt Cards:** Prepare basic documentation for the project. Create a **README** (or a markdown file) that explains how to set up and run the system locally. Include instructions like “Install Docker and run `docker-compose up` to start the app and LLM”. Document any prerequisites (for example, if the user needs to download a model or if the Docker setup will handle it automatically). Also include a user guide: explain how to use the UI to create prompt cards, add test cases, and run tests. Mention that the LLM is running locally via Ollama and which model is included by default. Provide a couple of **example prompt cards** in the documentation (and/or include them pre-seeded in the app). For instance, give an example YAML for a prompt card and explain how it works. Ensure the documentation covers how to interpret test results (e.g. what a ✅ vs ❌ means, how to add assertions in test cases for automatic checking, etc.). Keep the language concise and developer-friendly, perhaps with bullet points or step-by-step usage instructions.
* **Testing & QA:** Do a thorough run-through of the entire application as if you were the end user and a developer reviewing the code. Verify the following:

  * All CRUD operations work (create/edit/delete prompt cards, and they reflect in the UI without page errors).
  * Running tests produces results consistently and the results are correct for the given inputs (at least qualitatively – e.g. if testing a translation prompt, verify the output is indeed a translation).
  * The system can handle edge cases gracefully (no test cases, extremely long prompt text, special characters in input, etc.).
  * The Docker Compose deployment works from scratch on a different machine: i.e., a new developer or user can clone the repo, run `docker-compose up`, and everything (frontend, backend, Ollama) comes up properly. This might uncover any missing setup steps (for example, if the Ollama container needs a model downloaded, ensure this is either included or clearly documented).
  * If time permits, write a few basic automated tests for the backend and/or frontend. For instance, you could write a small script to call the evaluate API with a known prompt and use a dummy provider (Promptfoo has an `echo` provider or similar for testing) to verify the mechanism without invoking the real model. Or add a unit test for the YAML parser utility to ensure it outputs expected structures. This is optional but adds reliability.
* **Final Bug Fixes:** Fix any bugs discovered during testing. Typical fixes might include: adjusting timeouts if results take too long, cleaning up any console errors in the browser, ensuring database writes are committed, etc. Also, review the code for any hard-coded values or secrets – since it’s all local, there likely aren’t sensitive secrets, but for example, if an API key was needed for a provider, ensure it’s not in code (not applicable for Ollama local model). Make sure the error handling won’t expose confusing stack traces to users; instead log errors server-side and return friendly messages.
* **Project Handoff:** Prepare the final deliverables. This includes the code repository (with clear commit history and the README), the Docker Compose configuration, and any supporting files (like example YAMLs or a small dataset of prompt cards, if applicable). Double-check that all requirement points are met: UI for creating/editing prompt cards ✔, defining test cases ✔, running tests via Promptfoo ✔, viewing results ✔, YAML parsing ✔, example prompt cards ✔, basic documentation ✔. Since there’s no external deadline, use any remaining time to refine or add minor enhancements that boost usability or clarity in the system.

**Responsibilities:**

* *Frontend Dev:* Focus on improving the user interface/experience this week – making the app intuitive and fixing any UI bugs. Implement the import/export YAML UI elements. Assist with documentation by providing usage notes or screenshots of the UI if helpful.
* *Backend Dev:* Finalize all backend features such as YAML import/export handling, edge-case handling for the evaluate function (e.g. what if no providers or no tests – ensure it doesn’t crash), and optimize any slow queries or memory issues. This dev should also verify that the Dockerized environment is production-ready (for local use) – e.g. ensure the Express server in Docker uses proper settings (maybe using production build of Next.js if relevant).
* *DevOps/Integrator:* Verify the end-to-end deployment. This includes testing the Docker Compose from a clean state, and possibly creating a one-step startup script. Also, contribute to documentation on setup and any configuration toggles (for example, how to switch to a different model in Ollama, or how to update the system if needed).
* *Whole Team:* Collaboratively test the application. It’s beneficial to have each developer use the other’s components (e.g. a frontend dev testing the API via Swagger or curl, a backend dev running through the UI) to catch issues from different perspectives. Review each other’s code and ensure consistency (coding style, comments for complex sections like the Promptfoo integration). Everyone should contribute to the documentation to make sure it’s accurate and clear. If the team is just one person, allocate time to step away and then do a fresh run-through, simulating a new user’s experience. Also consider getting feedback from a colleague or friend on the usability.

**Dependencies:**

* All major features from previous weeks must be in place. This week depends on having a mostly working application to polish; if any core feature is incomplete (e.g. test running), it will take priority to finish before polish.
* Feedback from testing is crucial. Ensure that any testers (team members or others) have the environment set up to run the app and log issues. Use this feedback to drive the final fixes.
* Documentation writing may require information from all parts of the system. For instance, details about the Promptfoo integration or usage of YAML should be gathered (like noting that the YAML format corresponds to Promptfoo’s config format, so advanced users can tweak tests outside the UI if desired). Cite the Promptfoo usage where relevant, so users know the standards (e.g. mention that under the hood the app uses Promptfoo’s eval, which uses YAML configs).
* Time management is important: allocate a few days to documentation and final testing. Avoid adding new major features this late; stick to ensuring the MVP meets all listed requirements reliably. Since there’s no external hard deadline, quality is more important than rushing. However, aim to finish within the 30-day window to maintain momentum and demonstrate steady progress as planned.

**Deliverables (End of Week 4):** By the end of this week, we expect a functional MVP delivered with the following:

* **Codebase:** containing the Next.js frontend and Express backend, configured via Docker Compose for local deployment.
* **Functionality:** Users can create and edit prompt cards (with prompt templates and variables), define test cases, run those tests against a local LLM via Promptfoo, and see the results.
* **YAML Support:** The system can import and export prompt/test configurations in YAML (compatible with Promptfoo’s format) to facilitate manual editing or integration with other tools.
* **Example Prompts:** A few sample prompt cards are included (or documented) to showcase usage (e.g. translation prompt example, summarization prompt example, etc.).
* **Docs:** A README or documentation file explaining setup and usage, so that developers or users can understand how to run the system and what each component does. This will include how to start the Docker environment, how to add new models to Ollama if needed, and how to run tests with Promptfoo (mentioning any relevant config like default port 11434 for Ollama).
* **Next Steps (Optional):** Although beyond MVP, note in docs or comments any ideas for future improvement (such as a richer test assertion UI, multi-model comparisons, or a cloud deployment) – this shows forethought but isn’t required to implement now.

With all of the above, the Prompt Card MVP system will be complete and ready for use locally, achieving the goal of test-driven prompt development with a self-hosted LLM and evaluation harness. Each week’s structured progress ensures that by Day 30, the core features are stable and documented, fulfilling the project requirements and setting a foundation for potential future enhancements.
